import os
from langchain_openai import ChatOpenAI
from typing import TypedDict
from typing_extensions import Annotated
from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ToolMessage  # æ·»åŠ  ToolMessage
import operator
from langgraph.graph import StateGraph, END
from langchain_community.tools.tavily_search import TavilySearchResults
from tools import MessagerManager
from tools.document_exporter import create_document_export_tool



class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], MessagerManager(max_woking_memory=100,max_history=500)]

 
class Agent:
    def __init__(self, model, tools, system=""):
        self.system = system
        self.model = model
        self.tools = {t.name: t for t in tools}
        self.message_manager = MessagerManager(max_woking_memory=100, max_history=500)
        # ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
        self.model = self.model.bind_tools(tools)
        
        # æ„å»ºå·¥ä½œæµå›¾
        graph = StateGraph(AgentState)  
        graph.add_node("llm", self.call_openai)
        graph.add_node("action", self.take_action)
        graph.add_conditional_edges(
            "llm",
            self.exists_action,
            {
                True: "action",
                False: END
            }
        )
        graph.add_edge("action", "llm")
        graph.set_entry_point("llm")  # æ­£ç¡®çš„å…¥å£ç‚¹è®¾ç½®
        self.graph = graph.compile()
    
    def exists_action(self,state:AgentState):
        """
        æ£€æŸ¥å·¥å…·è°ƒç”¨
        """

        if not state["messages"]:
            return False
        last_message = state["messages"][-1]
        if isinstance(last_message,AIMessage):
            return hasattr(last_message,'tool_calls') and len(last_message.tool_calls) > 0
        if isinstance(last_message, ToolMessage):
            return False
        return False

    def call_openai(self, state:AgentState):
        messages = state["messages"]
        if self.system:
            # ç¡®ä¿ç³»ç»Ÿæ¶ˆæ¯åœ¨æœ€å‰é¢
            system_msg = [SystemMessage(content=self.system)]
            non_system_msgs = [msg for msg in messages if not isinstance(msg, SystemMessage)]
            messages = system_msg + non_system_msgs
        
        print(f"å‘é€ç»™æ¨¡å‹çš„æ¶ˆæ¯æ•°é‡: {len(messages)}")
        response = self.model.invoke(messages)
        return {"messages": [response]}
    def take_action(self , state:AgentState):
        tool_calls = state["messages"][-1].tool_calls
        results = []
        for t in tool_calls:
            print(f"Calling: {t}")
            if not t['name'] in self.tools:
                print("\n ....bad tool name....")
                result = "bad tool name, retry"
            else:
                result = self.tools[t['name']].invoke(t['args'])
            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))
        print("Back to the model!")
        return {'messages': results}


if __name__ == "__main__":

    os.environ["TAVILY_API_KEY"] = "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"
    
    # åˆ›å»ºå·¥å…·åˆ—è¡¨
    search_tool = TavilySearchResults(max_results=2)
    document_tool = create_document_export_tool()
    tools = [search_tool, document_tool]

    prompt ="""
    ä½ æ˜¯ä¸€åèªæ˜çš„ç§‘ç ”åŠ©ç†ã€‚ä½ æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š
    
    1. ğŸ” æœç´¢å¼•æ“ï¼šå¯ä»¥æœç´¢æœ€æ–°ä¿¡æ¯
    2. ğŸ“„ æ–‡æ¡£å¯¼å‡ºï¼šå¯ä»¥å°†å†…å®¹ä¿å­˜ä¸º Markdown æ–‡æ¡£
    
    ä½¿ç”¨æŒ‡å—ï¼š
    - å½“ç”¨æˆ·éœ€è¦æœç´¢ä¿¡æ¯æ—¶ï¼Œä½¿ç”¨æœç´¢å·¥å…·
    - å½“ç”¨æˆ·è¦æ±‚"æ•´ç†åˆ°æ–‡æ¡£"ã€"ä¿å­˜åˆ°æ–‡ä»¶"ã€"å¯¼å‡ºæŠ¥å‘Š"ç­‰æ—¶ï¼Œä½¿ç”¨æ–‡æ¡£å¯¼å‡ºå·¥å…·
    - å¯ä»¥å…ˆæœç´¢ä¿¡æ¯ï¼Œç„¶åå°†ç»“æœæ•´ç†å¯¼å‡ºåˆ°æ–‡æ¡£
    
    æ–‡æ¡£å¯¼å‡ºæ ¼å¼ï¼š
    - å¦‚æœç”¨æˆ·æŒ‡å®šäº†æ ‡é¢˜ï¼Œä½¿ç”¨æ ¼å¼ï¼š"æ ‡é¢˜|å†…å®¹"
    - å¦‚æœæ²¡æœ‰æŒ‡å®šæ ‡é¢˜ï¼Œç›´æ¥ä¼ å…¥å†…å®¹å³å¯
    
    ä½ å¯ä»¥å¤šæ¬¡è°ƒç”¨å·¥å…·ï¼Œä¹Ÿå¯ä»¥ç»„åˆä½¿ç”¨ï¼ˆæ¯”å¦‚å…ˆæœç´¢ï¼Œå†å¯¼å‡ºï¼‰ã€‚
    """.strip()

    model = ChatOpenAI(
        model = "Qwen/Qwen3-Coder-480B-A35B-Instruct",
        api_key = 'ms-15b6023d-3719-4505-ac95-ebffd78deec5',
        base_url = 'https://api-inference.modelscope.cn/v1/'
    )

    abot = Agent(model, tools, prompt)

    print("ğŸ¤– AIåŠ©æ‰‹å·²å¯åŠ¨ï¼è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºå¯¹è¯\n")

    state = {"messages":[]}
    while True:
        try:
            user_input = input("ğŸ‘¤ ç”¨æˆ·: ").strip()

            if user_input.lower() in ['quit', 'exit','q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            if not user_input:
                continue

            state["messages"].append(HumanMessage(content=user_input))

            result = abot.graph.invoke(state)

            ai_res = result['messages'][-1].content
            print(f"ğŸ¤– AI: {ai_res}")

            state["messages"] = result['messages']
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·é‡è¯•æˆ–è¾“å…¥ 'quit' é€€å‡º")