import os
from langchain_openai import ChatOpenAI
from typing import TypedDict, List, Dict, Any
from typing_extensions import Annotated
from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ToolMessage
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import InMemorySaver
from agents.BaseAgent import BaseAgent,TaskEvaluatorAgent,TaskExecutorAgent,TaskPlannerAgent
from agents.MessageManager import MessagerManager
from agents.MultiAgentState import MultiAgentState
from tools.TavilySearcher import create_tavily_search_reader_tool
from tools.document_exporter import create_document_export_tool
from tools.DocumentReader import create_document_reader_tool
from tools.Path_Acquire import create_path_acquire_tool
<<<<<<< HEAD

=======
from tools.RAGRetriever import create_rag_tools
from tools.clean_think import clean_response
>>>>>>> ollama_use_meta_chunk

# å¤šæ™ºèƒ½ä½“ç®¡ç†å™¨
class MultiAgent:
    def __init__(self, model, tools):
        self.model = model
        self.tools = tools
        
        # åˆ›å»ºä¸‰ä¸ªä¸“é—¨åŒ–æ™ºèƒ½ä½“
        self.planner = TaskPlannerAgent(model, tools)  # ä¼ é€’å·¥å…·ä¿¡æ¯ä½†ä¸ç»‘å®š
        self.executor = TaskExecutorAgent(model, tools) 
        self.evaluator = TaskEvaluatorAgent(model, [])
        
        self.message_manager = MessagerManager(max_woking_memory=100, max_history=500)
        
<<<<<<< HEAD
        # åˆ›å»ºæ£€æŸ¥ç‚¹ä¿å­˜å™¨å®ç°è®°å¿†åŠŸèƒ½
        self.checkpointer = InMemorySaver()
        
=======
>>>>>>> ollama_use_meta_chunk
        # æ„å»ºå¤šæ™ºèƒ½ä½“å·¥ä½œæµå›¾
        self.graph = self._build_workflow()
    
    def _build_workflow(self):
        """æ„å»ºå¤šæ™ºèƒ½ä½“å·¥ä½œæµ - æ”¯æŒå¤šå·¥å…·å¾ªç¯æ‰§è¡Œ"""
        workflow = StateGraph(MultiAgentState)
        
        # æ·»åŠ æ™ºèƒ½ä½“èŠ‚ç‚¹
        workflow.add_node("planner", self._planner_node)
        workflow.add_node("executor", self._executor_node)
        workflow.add_node("evaluator", self._evaluator_node)
        workflow.add_node("tool_execution", self._tool_execution_node)
        
        # è®¾ç½®å·¥ä½œæµç¨‹ - æ”¯æŒå¾ªç¯æ‰§è¡Œ
        workflow.add_edge("planner", "executor")
        
        # æ‰§è¡Œè€…å¯èƒ½éœ€è¦å·¥å…·è°ƒç”¨
        workflow.add_conditional_edges(
            "executor",
            self._needs_tool_execution,
            {
                True: "tool_execution",
                False: "evaluator"
            }
        )
        
        # å·¥å…·æ‰§è¡Œåï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æ‰§è¡Œæ›´å¤šå·¥å…·
        workflow.add_conditional_edges(
            "tool_execution",
            self._should_continue_execution,
            {
                True: "executor",      # ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªå·¥å…·
                False: "evaluator"     # æ‰€æœ‰å·¥å…·æ‰§è¡Œå®Œæˆï¼Œè¿›å…¥è¯„ä¼°
            }
        )
        
        workflow.add_edge("evaluator", END)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("planner")
        
<<<<<<< HEAD
        return workflow.compile(checkpointer=self.checkpointer)
=======
        return workflow.compile()
>>>>>>> ollama_use_meta_chunk
    
    def _planner_node(self, state: MultiAgentState) -> Dict:
        """ä»»åŠ¡è§„åˆ’èŠ‚ç‚¹"""
        print(f"\nğŸ¯ {self.planner.name} å¼€å§‹åˆ†æä»»åŠ¡...")
        
<<<<<<< HEAD
        result = self.planner.process(state)
        response = result["response"]
        
        # åˆ†æä»»åŠ¡è®¡åˆ’ä¸­éœ€è¦çš„å·¥å…·è°ƒç”¨
        planned_tool_calls = self._extract_planned_tool_calls(response.content)
        planned_tools = list(set([call['name'] for call in planned_tool_calls]))  # å»é‡è·å–å·¥å…·åç§°åˆ—è¡¨
=======
        # è·å–ç”¨æˆ·æŸ¥è¯¢
        user_query = state.get("user_query", "")
        
        # ç›´æ¥æ£€æŸ¥æ˜¯å¦æ˜¯å·¥å…·è°ƒç”¨
        available_tools = {tool.name: tool for tool in self.tools}
        direct_tool_call = None
        
        # æ£€æŸ¥ç”¨æˆ·æŸ¥è¯¢æ˜¯å¦ç›´æ¥æ˜¯å·¥å…·åç§°
        if user_query.strip() in available_tools:
            direct_tool_call = {
                'name': user_query.strip(),
                'params': {},
                'step': 1
            }
        else:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·åç§°
            for tool_name in available_tools.keys():
                if tool_name in user_query:
                    # å°è¯•æå–å‚æ•°
                    params = {}
                    tool_part = user_query.split(tool_name, 1)[1].strip() # è·å–å·¥å…·ååçš„éƒ¨åˆ†
                    if tool_part:
                        # ç®€å•å¤„ç†ï¼šå¦‚æœåé¢è·Ÿçš„æ˜¯æ–‡ä»¶è·¯å¾„æˆ–ç®€å•å­—ç¬¦ä¸²ï¼Œä½œä¸ºä¸»è¦å‚æ•°
                        # è¿™é‡Œå¯ä»¥æ ¹æ®å·¥å…·çš„å…·ä½“éœ€æ±‚è¿›è¡Œæ›´å¤æ‚çš„è§£æ
                        # ä¾‹å¦‚ï¼Œå¯¹äº add_document_to_ragï¼ŒæœŸæœ›ä¸€ä¸ª file_path å‚æ•°
                        # å¯¹äº rag_question_answerï¼ŒæœŸæœ›ä¸€ä¸ª question å‚æ•°
                        # æˆ‘ä»¬å¯ä»¥åšä¸€ä¸ªé€šç”¨çš„å¤„ç†ï¼ŒæŠŠå‰©ä½™éƒ¨åˆ†ä½œä¸º 'input' å‚æ•°
                        # æˆ–è€…æ ¹æ®å·¥å…·ååšç‰¹å®šå¤„ç†
                        if tool_name == "rag_question_answer":
                             # å¯¹äº rag_question_answerï¼Œå‰©ä½™éƒ¨åˆ†æ˜¯é—®é¢˜
                             params["question"] = tool_part
                        elif tool_name in ["add_document_to_rag", "add_directory_to_rag", "delete_rag_document"]:
                            # å¯¹äºè¿™äº›å·¥å…·ï¼Œå‰©ä½™éƒ¨åˆ†å¾ˆå¯èƒ½æ˜¯è·¯å¾„
                            # ç§»é™¤å¯èƒ½çš„å¼•å·
                            path = tool_part.strip('\'"')
                            params["file_path"] = path # æ³¨æ„ï¼šadd_directory_to_rag å®é™…éœ€è¦ directory_path, è¿™é‡Œç®€åŒ–å¤„ç†æˆ–éœ€è¦æ›´ç²¾ç¡®çš„æ˜ å°„
                        elif tool_name == "export_document":
                            # å¯¹äº export_documentï¼Œå‰©ä½™éƒ¨åˆ†å¯èƒ½æ˜¯å†…å®¹
                            params["content"] = tool_part
                        else:
                            # é€šç”¨å¤„ç†ï¼Œå°†å‰©ä½™éƒ¨åˆ†ä½œä¸º 'input' å‚æ•°
                            params["input"] = tool_part
                    
                    direct_tool_call = {
                        'name': tool_name,
                        'params': params, # ä½¿ç”¨è§£æå‡ºçš„å‚æ•°
                        'step': 1
                    }
                    break
        
        if direct_tool_call:
            # ç›´æ¥å·¥å…·è°ƒç”¨ï¼Œè·³è¿‡AIè§„åˆ’
            response_content = f"ç›´æ¥è°ƒç”¨å·¥å…·: {direct_tool_call['name']}"
            response = AIMessage(content=response_content)
            planned_tool_calls = [direct_tool_call]
            planned_tools = [direct_tool_call['name']]
        else:
            # æ­£å¸¸AIè§„åˆ’æµç¨‹
            result = self.planner.process(state)
            response = result["response"]
            # è¾“å‡ºåŠ å·¥
            clean_response(response)
            # åˆ†æä»»åŠ¡è®¡åˆ’ä¸­éœ€è¦çš„å·¥å…·è°ƒç”¨
            planned_tool_calls = self._extract_planned_tool_calls(response.content)
            planned_tools = list(set([call['name'] for call in planned_tool_calls]))  # å»é‡è·å–å·¥å…·åç§°åˆ—è¡¨
>>>>>>> ollama_use_meta_chunk
        
        # æ›´æ–°çŠ¶æ€
        state["messages"].append(response)
        state["current_agent"] = self.planner.name
        state["task_plan"] = response.content
        state["step"] = "planning_complete"
<<<<<<< HEAD
        state["agent_history"].append(result["agent_record"])
=======
        if not direct_tool_call:
            state["agent_history"].append(result["agent_record"])
>>>>>>> ollama_use_meta_chunk
        state["planned_tools"] = planned_tools
        state["executed_tools"] = []
        state["planned_tool_calls"] = planned_tool_calls
        state["executed_tool_calls"] = []
        state["execution_count"] = 0
        state["current_tool_call_index"] = 0
        
        print(f"ğŸ“‹ ä»»åŠ¡è®¡åˆ’ï¼š\n{response.content}")
        print(f"ğŸ› ï¸ è®¡åˆ’ä½¿ç”¨çš„å·¥å…·ï¼š{planned_tools}")
        return state
    
    def _executor_node(self, state: MultiAgentState) -> Dict:
        """ä»»åŠ¡æ‰§è¡ŒèŠ‚ç‚¹"""
        print(f"\nâš¡ {self.executor.name} å¼€å§‹æ‰§è¡Œä»»åŠ¡...")
        
        # æ›´æ–°æ‰§è¡Œè®¡æ•°
        state["execution_count"] = state.get("execution_count", 0) + 1
        
        # ç¡®å®šä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„å·¥å…·è°ƒç”¨
        next_tool_call = self._determine_next_tool_call(state)
        executed_calls = state.get("executed_tool_calls", [])
        
        # ä¸ºæ‰§è¡Œè€…æä¾›æ˜ç¡®çš„æŒ‡å¯¼
        if next_tool_call:
            # æ ¼å¼åŒ–å‚æ•°å­—ç¬¦ä¸²
            params_str = ", ".join([f'{k}="{v}"' for k, v in next_tool_call['params'].items()])
            
            guided_context = f"""
æ ¹æ®ä»»åŠ¡è®¡åˆ’ï¼Œä½ éœ€è¦æŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹å·¥å…·è°ƒç”¨ï¼š

å½“å‰çŠ¶æ€ï¼š
- å·²æ‰§è¡Œçš„å·¥å…·è°ƒç”¨ï¼š{len(executed_calls)} ä¸ª
- ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„å·¥å…·è°ƒç”¨ï¼š{next_tool_call['name']}({params_str})

é‡è¦æŒ‡ç¤ºï¼š
1. ç°åœ¨åªæ‰§è¡Œè¿™ä¸€ä¸ªå·¥å…·è°ƒç”¨ï¼š{next_tool_call['name']}
2. ä½¿ç”¨ä»¥ä¸‹å‚æ•°ï¼š{next_tool_call['params']}
3. ä¸è¦åŒæ—¶è°ƒç”¨å¤šä¸ªå·¥å…·
4. ä¸“æ³¨äºå½“å‰æ­¥éª¤ï¼Œæ‰§è¡Œå®Œæˆåç³»ç»Ÿä¼šè‡ªåŠ¨è®©ä½ ç»§ç»­ä¸‹ä¸€æ­¥

è¯·ç«‹å³è°ƒç”¨ {next_tool_call['name']} å·¥å…·ï¼Œä½¿ç”¨æŒ‡å®šçš„å‚æ•°ã€‚
"""
            # åˆ›å»ºæŒ‡å¯¼æ¶ˆæ¯
            guidance_message = HumanMessage(content=guided_context)
            temp_state = state.copy()
            temp_state["messages"] = state["messages"] + [guidance_message]
            
            result = self.executor.process(temp_state)
            response = result["response"]
            
            # æ·»åŠ æŒ‡å¯¼æ¶ˆæ¯åˆ°çŠ¶æ€
            state["messages"].append(guidance_message)
        else:
            # æ²¡æœ‰æ›´å¤šå·¥å…·è°ƒç”¨éœ€è¦æ‰§è¡Œï¼Œè®©æ‰§è¡Œè€…æ€»ç»“å½“å‰çŠ¶æ€
            executed_calls = state.get("executed_tool_calls", [])
            planned_calls = state.get("planned_tool_calls", [])
            
            summary_context = f"""
æ‰€æœ‰è®¡åˆ’çš„å·¥å…·è°ƒç”¨éƒ½å·²æ‰§è¡Œå®Œæˆã€‚

æ‰§è¡Œæ€»ç»“ï¼š
- è®¡åˆ’çš„å·¥å…·è°ƒç”¨ï¼š{len(planned_calls)} ä¸ª
- å·²æ‰§è¡Œçš„å·¥å…·è°ƒç”¨ï¼š{len(executed_calls)} ä¸ª

è¯·æ€»ç»“æ‰§è¡Œç»“æœï¼Œä¸ºè¯„ä¼°é˜¶æ®µåšå‡†å¤‡ã€‚
"""
            guidance_message = HumanMessage(content=summary_context)
            temp_state = state.copy()
            temp_state["messages"] = state["messages"] + [guidance_message]
            
            result = self.executor.process(temp_state)
            response = result["response"]
<<<<<<< HEAD
=======
            # è¾“å‡ºåŠ å·¥
            clean_response(response)
>>>>>>> ollama_use_meta_chunk
            
            # æ·»åŠ æŒ‡å¯¼æ¶ˆæ¯åˆ°çŠ¶æ€
            state["messages"].append(guidance_message)
        
        # æ›´æ–°çŠ¶æ€
        state["messages"].append(response)
        state["current_agent"] = self.executor.name
        
        # ç´¯ç§¯æ‰§è¡Œç»“æœ
        if state.get("execution_result"):
            state["execution_result"] += f"\n\næ­¥éª¤ {state['execution_count']}:\n{response.content}"
        else:
            state["execution_result"] = response.content
            
        state["step"] = "execution_complete"
        state["agent_history"].append(result["agent_record"])
        
        print(f"ğŸ”§ æ‰§è¡Œè¿‡ç¨‹ï¼š\n{response.content}")
        return state
    
    def _tool_execution_node(self, state: MultiAgentState) -> Dict:
        """å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹"""
        print(f"\nğŸ› ï¸ æ‰§è¡Œå·¥å…·è°ƒç”¨...")
        
        last_message = state["messages"][-1]
        if isinstance(last_message, AIMessage) and hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            tool_results = []
            executed_tools = state.get("executed_tools", [])
            executed_tool_calls = state.get("executed_tool_calls", [])
            
            for tool_call in last_message.tool_calls:
                print(f"è°ƒç”¨å·¥å…·: {tool_call['name']} å‚æ•°: {tool_call['args']}")
                
                # è®°å½•å·²æ‰§è¡Œçš„å·¥å…·åç§°ï¼ˆå»é‡ï¼‰
                if tool_call['name'] not in executed_tools:
                    executed_tools.append(tool_call['name'])
                
                # è®°å½•è¯¦ç»†çš„å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆä¸å»é‡ï¼Œå…è®¸åŒä¸€å·¥å…·å¤šæ¬¡è°ƒç”¨ï¼‰
                executed_tool_calls.append({
                    'name': tool_call['name'],
                    'args': tool_call['args'],
                    'call_id': tool_call['id'],
                    'step': len(executed_tool_calls) + 1
                })
                
                if tool_call['name'] in self.executor.tools:
                    try:
<<<<<<< HEAD
                        result = self.executor.tools[tool_call['name']].invoke(tool_call['args'])
=======
                        # å¯¹äºæ— å‚æ•°çš„å·¥å…·ï¼Œä¼ å…¥ç©ºå­—ç¬¦ä¸²
                        if not tool_call['args'] or tool_call['args'] == {}:
                            result = self.executor.tools[tool_call['name']].invoke("")
                        else:
                            result = self.executor.tools[tool_call['name']].invoke(tool_call['args'])
>>>>>>> ollama_use_meta_chunk
                        tool_results.append(ToolMessage(
                            tool_call_id=tool_call['id'],
                            name=tool_call['name'],
                            content=str(result)
                        ))
                        print(f"âœ… å·¥å…· {tool_call['name']} æ‰§è¡ŒæˆåŠŸ")
<<<<<<< HEAD
                        print(f"å·¥å…·ç»“æœ: {str(result)[:200]}...")
=======
                        # å¯¹äº get_rag_stats å·¥å…·ï¼Œæ˜¾ç¤ºå®Œæ•´ç»“æœ
                        if tool_call['name'] == 'get_rag_stats':
                            print(f"å·¥å…·ç»“æœ:\n{str(result)}")
                        else:
                            print(f"å·¥å…·ç»“æœ: {str(result)[:500]}...")
>>>>>>> ollama_use_meta_chunk
                    except Exception as e:
                        tool_results.append(ToolMessage(
                            tool_call_id=tool_call['id'],
                            name=tool_call['name'],
                            content=f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
                        ))
                        print(f"âŒ å·¥å…· {tool_call['name']} æ‰§è¡Œå¤±è´¥: {str(e)}")
                else:
                    tool_results.append(ToolMessage(
                        tool_call_id=tool_call['id'],
                        name=tool_call['name'],
                        content="æœªçŸ¥å·¥å…·"
                    ))
                    print(f"âŒ æœªçŸ¥å·¥å…·: {tool_call['name']}")
            
            # æ›´æ–°å·²æ‰§è¡Œå·¥å…·åˆ—è¡¨å’Œè°ƒç”¨åˆ—è¡¨
            state["executed_tools"] = executed_tools
            state["executed_tool_calls"] = executed_tool_calls
            
            # æ›´æ–°å½“å‰å·¥å…·è°ƒç”¨ç´¢å¼•
            state["current_tool_call_index"] = state.get("current_tool_call_index", 0) + len(last_message.tool_calls)
            
            # ä½¿ç”¨ MessageManager æ™ºèƒ½ç®¡ç†æ¶ˆæ¯æ·»åŠ 
            current_messages = state["messages"]
            managed_messages = self.message_manager(current_messages, tool_results)
            state["messages"] = managed_messages
            state["step"] = "tool_execution_complete"
            
            # æ˜¾ç¤ºæ‰§è¡Œè¿›åº¦
            self._show_tool_call_progress(state)
        
        return state
    
    def _evaluator_node(self, state: MultiAgentState) -> Dict:
        """ç»“æœè¯„ä¼°èŠ‚ç‚¹"""
        print(f"\nğŸ” {self.evaluator.name} å¼€å§‹è¯„ä¼°ç»“æœ...")
        
        result = self.evaluator.process(state)
        response = result["response"]
<<<<<<< HEAD
        
=======
        # è¾“å‡ºåŠ å·¥
        clean_response(response)
>>>>>>> ollama_use_meta_chunk
        # æ›´æ–°çŠ¶æ€
        state["messages"].append(response)
        state["current_agent"] = self.evaluator.name
        state["evaluation_result"] = response.content
        state["step"] = "evaluation_complete"
        state["completed"] = True
        state["agent_history"].append(result["agent_record"])
        
        print(f"ğŸ“Š è¯„ä¼°ç»“æœï¼š\n{response.content}")
        return state
    
    def _extract_planned_tool_calls(self, task_plan: str) -> List[Dict]:
        """ä»ä»»åŠ¡è®¡åˆ’ä¸­æå–è¯¦ç»†çš„å·¥å…·è°ƒç”¨ä¿¡æ¯"""
        import re
        
        # è·å–æ‰€æœ‰å¯ç”¨å·¥å…·çš„åç§°
        available_tools = {tool.name: tool for tool in self.tools}
        tool_calls = []
        
        # æ–¹æ³•1ï¼šä»"å·¥å…·è°ƒç”¨æ¸…å•ï¼š"éƒ¨åˆ†æå–
        calls_section_pattern = r'å·¥å…·è°ƒç”¨æ¸…å•[ï¼š:]\s*\n((?:- .+\n?)*)'
        calls_match = re.search(calls_section_pattern, task_plan, re.MULTILINE)
        
        if calls_match:
            calls_text = calls_match.group(1)
            # è§£ææ¯ä¸ªå·¥å…·è°ƒç”¨ - ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼ä»¥æ”¯æŒä¸‹åˆ’çº¿
            call_pattern = r'- ([a-zA-Z_][a-zA-Z0-9_]*)\(([^)]+)\)'
            for match in re.finditer(call_pattern, calls_text):
                tool_name = match.group(1)
                params_str = match.group(2)
                
                if tool_name in available_tools:
                    # è§£æå‚æ•° - æ”¹è¿›å‚æ•°è§£æé€»è¾‘
                    params = {}
                    
                    # å¤„ç†å¤šç§å‚æ•°æ ¼å¼
                    if '=' in params_str:
                        # æ ¼å¼: key="value", key2="value2"
                        param_pattern = r'(\w+)="([^"]*)"'
                        for param_match in re.finditer(param_pattern, params_str):
                            param_name = param_match.group(1)
                            param_value = param_match.group(2)
                            params[param_name] = param_value
                    else:
                        # ç®€å•æ ¼å¼ï¼Œå¯èƒ½åªæœ‰ä¸€ä¸ªå‚æ•°å€¼
                        # å»æ‰å¼•å·
                        clean_params = params_str.strip().strip('"\'')
                        # æ ¹æ®å·¥å…·ç±»å‹æ¨æ–­å‚æ•°å
                        if tool_name == 'tavily_search_results_json':
                            params['query'] = clean_params
                        elif tool_name == 'document_reader':
                            params['file_path'] = clean_params
                        elif tool_name == 'export_document':
                            params['content'] = clean_params
                    
                    tool_calls.append({
                        'name': tool_name,
                        'params': params,
                        'step': len(tool_calls) + 1
                    })
        
        # æ–¹æ³•2ï¼šä»"å·¥å…·è°ƒç”¨ï¼š"éƒ¨åˆ†æå–ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
        if not tool_calls:
            call_pattern = r'å·¥å…·è°ƒç”¨[ï¼š:]\s*([a-zA-Z_][a-zA-Z0-9_]*)\(([^)]+)\)'
            for match in re.finditer(call_pattern, task_plan):
                tool_name = match.group(1)
                params_str = match.group(2)
                
                if tool_name in available_tools:
                    params = {}
                    
                    # å¤„ç†å¤šç§å‚æ•°æ ¼å¼
                    if '=' in params_str:
                        param_pattern = r'(\w+)="([^"]*)"'
                        for param_match in re.finditer(param_pattern, params_str):
                            param_name = param_match.group(1)
                            param_value = param_match.group(2)
                            params[param_name] = param_value
                    else:
                        # ç®€å•æ ¼å¼å¤„ç†
                        clean_params = params_str.strip().strip('"\'')
                        if tool_name == 'tavily_search_results_json':
                            params['query'] = clean_params
                        elif tool_name == 'document_reader':
                            params['file_path'] = clean_params
                        elif tool_name == 'export_document':
                            params['content'] = clean_params
                    
                    tool_calls.append({
                        'name': tool_name,
                        'params': params,
                        'step': len(tool_calls) + 1
                    })
        
<<<<<<< HEAD
=======
        # æ–¹æ³•3ï¼šç›´æ¥åŒ¹é…ç”¨æˆ·è¾“å…¥ä¸­çš„å·¥å…·åç§°ï¼ˆæ–°å¢ï¼‰
        if not tool_calls:
            user_input = task_plan.strip()
            # æ£€æŸ¥ç”¨æˆ·è¾“å…¥æ˜¯å¦ç›´æ¥æ˜¯å·¥å…·åç§°
            if user_input in available_tools:
                tool_calls.append({
                    'name': user_input,
                    'params': {},
                    'step': 1
                })
            else:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·åç§°
                for tool_name in available_tools.keys():
                    if tool_name in user_input:
                        tool_calls.append({
                            'name': tool_name,
                            'params': {},
                            'step': len(tool_calls) + 1
                        })
                        break
        
>>>>>>> ollama_use_meta_chunk
        print(f"ğŸ” ä»ä»»åŠ¡è®¡åˆ’ä¸­æå–çš„å·¥å…·è°ƒç”¨: {len(tool_calls)} ä¸ª")
        for i, call in enumerate(tool_calls, 1):
            print(f"   {i}. {call['name']}({call['params']})")
        
        return tool_calls
    
    def _show_tool_call_progress(self, state: MultiAgentState):
        """æ˜¾ç¤ºå·¥å…·è°ƒç”¨æ‰§è¡Œè¿›åº¦"""
        planned_calls = state.get("planned_tool_calls", [])
        executed_calls = state.get("executed_tool_calls", [])
        
        if planned_calls:
            print(f"\nğŸ“Š å·¥å…·è°ƒç”¨æ‰§è¡Œè¿›åº¦:")
            for i, call in enumerate(planned_calls, 1):
                status = "âœ…" if i <= len(executed_calls) else "â³"
                params_str = ", ".join([f'{k}="{v}"' for k, v in call['params'].items()])
                print(f"   {i}. {call['name']}({params_str}) {status}")
            
            remaining = len(planned_calls) - len(executed_calls)
            if remaining > 0:
                print(f"   å‰©ä½™ {remaining} ä¸ªå·¥å…·è°ƒç”¨å¾…æ‰§è¡Œ")
            else:
                print(f"   ğŸ‰ æ‰€æœ‰å·¥å…·è°ƒç”¨æ‰§è¡Œå®Œæˆ!")
    
    def _get_executed_tools(self, state: MultiAgentState) -> List[str]:
        """è·å–å·²æ‰§è¡Œçš„å·¥å…·åˆ—è¡¨"""
        executed = []
        for msg in state["messages"]:
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                for tool_call in msg.tool_calls:
                    if tool_call['name'] not in executed:
                        executed.append(tool_call['name'])
        return executed
    
    def _determine_next_tool_call(self, state: MultiAgentState) -> Dict:
        """ç¡®å®šä¸‹ä¸€ä¸ªåº”è¯¥æ‰§è¡Œçš„å·¥å…·è°ƒç”¨"""
        planned_calls = state.get("planned_tool_calls", [])
        current_index = state.get("current_tool_call_index", 0)
        
        if current_index < len(planned_calls):
            return planned_calls[current_index]
        
        return None
    
    def _should_continue_execution(self, state: MultiAgentState) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­æ‰§è¡Œä»»åŠ¡"""
        # æ£€æŸ¥æ‰§è¡Œæ¬¡æ•°ï¼Œé¿å…æ— é™å¾ªç¯
        execution_count = state.get("execution_count", 0)
        if execution_count >= 5:
            print("âš ï¸ è¾¾åˆ°æœ€å¤§æ‰§è¡Œæ¬¡æ•°é™åˆ¶ï¼Œè½¬å…¥è¯„ä¼°é˜¶æ®µ")
            return False
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªæ‰§è¡Œçš„å·¥å…·è°ƒç”¨
        planned_calls = state.get("planned_tool_calls", [])
        executed_calls = state.get("executed_tool_calls", [])
        
        remaining_calls = len(planned_calls) - len(executed_calls)
        
        if remaining_calls > 0:
            print(f"ğŸ”„ è¿˜æœ‰ {remaining_calls} ä¸ªå·¥å…·è°ƒç”¨å¾…æ‰§è¡Œ")
            return True
        
        return False
    
    def _needs_tool_execution(self, state: MultiAgentState) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œå·¥å…·"""
        if not state["messages"]:
            return False
        
        last_message = state["messages"][-1]
        if isinstance(last_message, AIMessage):
            return hasattr(last_message, 'tool_calls') and len(last_message.tool_calls) > 0
        
        return False
    
<<<<<<< HEAD
    def process_query(self, user_query: str, thread_id: str = "default") -> Dict:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢ - æ”¯æŒä¼šè¯è®°å¿†"""
=======
    def process_query(self, user_query: str) -> Dict:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
>>>>>>> ollama_use_meta_chunk
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = MultiAgentState(
            messages=[HumanMessage(content=user_query)],
            current_agent="",
            task_plan="",
            execution_result="",
            evaluation_result="",
            user_query=user_query,
            agent_history=[],
            step="start",
            completed=False,
            execution_count=0,
            planned_tools=[],
            executed_tools=[],
            planned_tool_calls=[],
            executed_tool_calls=[],
            current_tool_call_index=0
        )
        
<<<<<<< HEAD
        # é…ç½®ä¼šè¯è®°å¿†
        config = {"configurable": {"thread_id": thread_id}}
        
        # è¿è¡Œå·¥ä½œæµï¼Œæ”¯æŒè®°å¿†åŠŸèƒ½
        final_state = initial_state
        for output in self.graph.stream(initial_state, config=config):
=======
        # è¿è¡Œå·¥ä½œæµ
        final_state = initial_state
        for output in self.graph.stream(initial_state):
>>>>>>> ollama_use_meta_chunk
            if isinstance(output, dict):
                final_state.update(output)
        
        return final_state



def run_multi_agent_mode() -> bool:
    """è¿è¡Œå¤šæ™ºèƒ½ä½“æ¨¡å¼""" 
<<<<<<< HEAD
    import uuid
    
    # åˆ›å»ºå·¥å…·åˆ—è¡¨
=======
    # åˆ›å»ºå·¥å…·åˆ—è¡¨
    print("åˆ›å»ºå·¥å…·åˆ—è¡¨")
>>>>>>> ollama_use_meta_chunk
    search_tool = create_tavily_search_reader_tool()
    document_export_tool = create_document_export_tool()
    document_reader_tool = create_document_reader_tool()
    path_ac_tool = create_path_acquire_tool()
<<<<<<< HEAD
    tools = [search_tool,document_export_tool, document_reader_tool,path_ac_tool]

    model = ChatOpenAI(
        model="Qwen/Qwen3-Coder-480B-A35B-Instruct",
        api_key='ms-15b6023d-3719-4505-ac95-ebffd78deec5',
        base_url='https://api-inference.modelscope.cn/v1/'
    )

    # åˆ›å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
    multi_agent = MultiAgent(model, tools)

    # ç”Ÿæˆä¼šè¯IDï¼Œå®ç°è®°å¿†åŠŸèƒ½
    session_id = str(uuid.uuid4())[:8]  # ä½¿ç”¨çŸ­çš„ä¼šè¯ID
    
=======
    print("åˆ›å»ºå·¥å…·åˆ—è¡¨å®Œæˆ")
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = ChatOpenAI(
            # model='Qwen/Qwen3-1.7B',
            # base_url='https://api-inference.modelscope.cn/v1',
            # api_key='ms-8b59067c-75ff-4b83-900e-26e00e46c531',
            # streaming=True  # ä½¿ç”¨æµå¼è°ƒç”¨ï¼Œå¯èƒ½ä¸éœ€è¦enable_thinkingå‚æ•°
            
            model='qwen3:1.7b',
            base_url='http://localhost:11434/v1',
            api_key='ollama',
            streaming=True
        )
        
    # åˆ›å»ºRAGå·¥å…·ï¼ˆä¼ å…¥æ¨¡å‹å®ä¾‹ï¼‰
    print("åˆ›å»ºRAGå·¥å…·")
    rag_tools = create_rag_tools(model=model)
    print("åˆ›å»ºRAGå·¥å…·å®Œæˆ")
    # åˆå¹¶æ‰€æœ‰å·¥å…·
    tools = [search_tool, document_export_tool, document_reader_tool, path_ac_tool] + rag_tools
    
    # åˆ›å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
    multi_agent = MultiAgent(model, tools)

>>>>>>> ollama_use_meta_chunk
    print("ğŸ¤– å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿå·²å¯åŠ¨ï¼")
    print("ğŸ“‹ ç³»ç»ŸåŒ…å«ä¸‰ä¸ªä¸“é—¨åŒ–æ™ºèƒ½ä½“ï¼š")
    print("   ğŸ¯ TaskPlanner - ä»»åŠ¡æ‹†è§£ä¸“å®¶")
    print("   âš¡ TaskExecutor - ä»»åŠ¡æ‰§è¡Œä¸“å®¶") 
    print("   ğŸ” TaskEvaluator - ç»“æœè¯„ä¼°ä¸“å®¶")
<<<<<<< HEAD
    print(f"\nğŸ§  å½“å‰ä¼šè¯ID: {session_id} (æ”¯æŒè®°å¿†åŠŸèƒ½)")
    print("ğŸ“ è¾“å…¥ 'new' åˆ›å»ºæ–°ä¼šè¯, 'æŸ¥çœ‹è®°å¿†' æŸ¥çœ‹å¯¹è¯å†å²")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºå¯¹è¯\n")

    while True:
        try:
            user_input = input(f"ğŸ‘¤ ç”¨æˆ·({session_id[:4]}): ").strip()
=======
    print("\nğŸ› ï¸ å¯ç”¨å·¥å…·ï¼š")
    print("   ğŸ” æœç´¢å·¥å…· - ç½‘ç»œä¿¡æ¯æ£€ç´¢")
    print("   ğŸ“„ æ–‡æ¡£å·¥å…· - æ–‡ä»¶è¯»å–å’Œå¯¼å‡º")
    print("   ğŸ“ è·¯å¾„å·¥å…· - æ–‡ä»¶è·¯å¾„è·å–")
    print("   ğŸ§  RAGå·¥å…· - æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    print("     â€¢ add_document_to_rag ./doc/ä¸­åäººæ°‘å…±å’Œå›½è¯åˆ¸æ³•(2019ä¿®è®¢).pdf - æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“")
    print("     â€¢ add_directory_to_rag ./docs/ - æ‰¹é‡æ·»åŠ ç›®å½•æ–‡æ¡£")
    print("     â€¢ rag_question_answer æ‚¨çš„é—®é¢˜ - åŸºäºçŸ¥è¯†åº“é—®ç­”")
    print("     â€¢ get_rag_stats - æŸ¥çœ‹çŸ¥è¯†åº“ç»Ÿè®¡")
    print("     â€¢ delete_rag_document ./path/to/file.md - åˆ é™¤æŒ‡å®šæ–‡æ¡£")
    print("     â€¢ clear_rag_knowledge_base - æ¸…ç©ºæ•´ä¸ªçŸ¥è¯†åº“")
    print("\nè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºå¯¹è¯\n")

    while True:
        try:
            user_input = input("ğŸ‘¤ ç”¨æˆ·: ").strip()
>>>>>>> ollama_use_meta_chunk

            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            if not user_input:
                continue
<<<<<<< HEAD
                
            # ç‰¹æ®Šå‘½ä»¤å¤„ç†
            if user_input.lower() == 'new':
                # åˆ›å»ºæ–°ä¼šè¯
                session_id = str(uuid.uuid4())[:8]
                print(f"ğŸ†• å·²åˆ›å»ºæ–°ä¼šè¯: {session_id}")
                continue
            elif user_input in ['æŸ¥çœ‹è®°å¿†', 'memory', 'history']:
                # æŸ¥çœ‹å¯¹è¯å†å²
                try:
                    config = {"configurable": {"thread_id": session_id}}
                    history = multi_agent.checkpointer.list(config)
                    if history:
                        print(f"\nğŸ“œ ä¼šè¯ {session_id} çš„å†å²è®°å¿†:")
                        for i, checkpoint in enumerate(history):
                            print(f"  {i+1}. æ£€æŸ¥ç‚¹ {checkpoint}")
                    else:
                        print(f"\nğŸ’­ ä¼šè¯ {session_id} æš‚æ— å†å²è®°å¿†")
                except Exception as e:
                    print(f"\nâš ï¸ æ— æ³•è·å–å†å²è®°å¿†: {e}")
                continue
=======
>>>>>>> ollama_use_meta_chunk

            print(f"\n{'='*60}")
            print(f"ğŸš€ å¼€å§‹å¤„ç†ä»»åŠ¡: {user_input}")
            print(f"{'='*60}")

<<<<<<< HEAD
            # å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œä¼ å…¥ä¼šè¯ID
            final_state = multi_agent.process_query(user_input, session_id)
=======
            # å¤„ç†ç”¨æˆ·æŸ¥è¯¢
            final_state = multi_agent.process_query(user_input)
>>>>>>> ollama_use_meta_chunk

            print(f"\n{'='*60}")
            print("âœ… ä»»åŠ¡å¤„ç†å®Œæˆï¼")
            print(f"{'='*60}")
            
            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            if final_state.get("evaluation_result"):
                print(f"\nğŸ“Š æœ€ç»ˆè¯„ä¼°ï¼š\n{final_state['evaluation_result']}")
            
            # æ˜¾ç¤ºæ™ºèƒ½ä½“åä½œå†å²
            """
            print(f"\nğŸ¤ æ™ºèƒ½ä½“åä½œå†å²ï¼š")
            for record in final_state.get("agent_history", []):
                print(f"   {record['agent']} ({record['role']}): {record['content'][:100]}...")
            """
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·é‡è¯•æˆ–è¾“å…¥ 'quit' é€€å‡º")

    return False


if __name__ == "__main__":
    print("ğŸ¤– LangGraph æ™ºèƒ½åŠ©æ‰‹ç³»ç»Ÿ")
    
    while True:
        if not run_multi_agent_mode():
            break
