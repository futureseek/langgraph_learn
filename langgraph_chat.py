import os
from langchain_openai import ChatOpenAI
from typing import TypedDict
from typing_extensions import Annotated
from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ToolMessage  # æ·»åŠ  ToolMessage
import operator
from langgraph.graph import StateGraph, END
from langchain_community.tools.tavily_search import TavilySearchResults
from tools import MessagerManager
from tools.document_exporter import create_document_export_tool
from tools.DocumentReader import create_document_reader_tool



class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], MessagerManager(max_woking_memory=100,max_history=500)]

 
class Agent:
    def __init__(self, model, tools, system=""):
        self.system = system
        self.model = model
        self.tools = {t.name: t for t in tools}
        self.message_manager = MessagerManager(max_woking_memory=100, max_history=500)
        # ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
        self.model = self.model.bind_tools(tools)
        
        # æ„å»ºå·¥ä½œæµå›¾
        graph = StateGraph(AgentState)  
        graph.add_node("llm", self.call_openai)
        graph.add_node("action", self.take_action)
        graph.add_conditional_edges(
            "llm",
            self.exists_action,
            {
                True: "action",
                False: END
            }
        )
        graph.add_edge("action", "llm")
        graph.set_entry_point("llm")  # æ­£ç¡®çš„å…¥å£ç‚¹è®¾ç½®
        self.graph = graph.compile()
    
    def exists_action(self,state:AgentState):
        """
        æ£€æŸ¥å·¥å…·è°ƒç”¨
        """

        if not state["messages"]:
            return False
        last_message = state["messages"][-1]
        if isinstance(last_message,AIMessage):
            return hasattr(last_message,'tool_calls') and len(last_message.tool_calls) > 0
        if isinstance(last_message, ToolMessage):
            return False
        return False

    def call_openai(self, state:AgentState):
        messages = state["messages"]
        if self.system:
            # ç¡®ä¿ç³»ç»Ÿæ¶ˆæ¯åœ¨æœ€å‰é¢
            system_msg = [SystemMessage(content=self.system)]
            non_system_msgs = [msg for msg in messages if not isinstance(msg, SystemMessage)]
            messages = system_msg + non_system_msgs
        
        print(f"å‘é€ç»™æ¨¡å‹çš„æ¶ˆæ¯æ•°é‡: {len(messages)}")
        response = self.model.invoke(messages)
        return {"messages": [response]}
    def take_action(self , state:AgentState):
        tool_calls = state["messages"][-1].tool_calls
        results = []
        for t in tool_calls:
            print(f"Calling: {t}")
            if not t['name'] in self.tools:
                print("\n ....bad tool name....")
                result = "bad tool name, retry"
            else:
                result = self.tools[t['name']].invoke(t['args'])
            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))
        print("Back to the model!")
        return {'messages': results}


if __name__ == "__main__":

    os.environ["TAVILY_API_KEY"] = "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"
    
    # åˆ›å»ºå·¥å…·åˆ—è¡¨
    search_tool = TavilySearchResults(max_results=2)
    document_export_tool = create_document_export_tool()
    document_reader_tool = create_document_reader_tool()
    tools = [search_tool, document_export_tool, document_reader_tool]

    prompt ="""
    ä½ æ˜¯ä¸€åæ™ºèƒ½æœºå™¨äººåŠ©æ‰‹ã€‚ä½ æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š
    
    ğŸ” ä¿¡æ¯è·å–ï¼š
    1. æœç´¢å¼•æ“ï¼šæœç´¢æœ€æ–°çš„ç½‘ç»œä¿¡æ¯
    2. æ–‡æ¡£è¯»å–ï¼šè¯»å–æœ¬åœ°æ–‡ä»¶å†…å®¹ï¼ˆæ”¯æŒ txt, md, py, js, json ç­‰å¤šç§æ ¼å¼ï¼‰
    
    ğŸ“ å†…å®¹å¤„ç†ï¼š
    3. æ–‡æ¡£å¯¼å‡ºï¼šå°†å†…å®¹ä¿å­˜ä¸º Markdown æ ¼å¼æ–‡æ¡£
    
    ğŸ’¡ ä½¿ç”¨æŒ‡å—ï¼š
    - å½“ç”¨æˆ·è¯¢é—®æœ¬åœ°æ–‡ä»¶å†…å®¹æ—¶ï¼Œä½¿ç”¨æ–‡æ¡£è¯»å–å·¥å…·
    - å½“ç”¨æˆ·éœ€è¦ç½‘ç»œæœç´¢æ—¶ï¼Œä½¿ç”¨æœç´¢å·¥å…·
    - å½“ç”¨æˆ·è¦æ±‚ä¿å­˜ã€å¯¼å‡ºã€æ•´ç†åˆ°æ–‡æ¡£æ—¶ï¼Œä½¿ç”¨æ–‡æ¡£å¯¼å‡ºå·¥å…·
    - å¯ä»¥ç»„åˆä½¿ç”¨ï¼šè¯»å–æœ¬åœ°æ–‡æ¡£ â†’ åˆ†æå†…å®¹ â†’ æœç´¢ç›¸å…³ä¿¡æ¯ â†’ å¯¼å‡ºç»¼åˆæŠ¥å‘Š
    
    ğŸ“„ æ–‡æ¡£è¯»å–æ”¯æŒï¼š
    - å•ä¸ªæ–‡ä»¶ï¼šç›´æ¥æä¾›æ–‡ä»¶è·¯å¾„
    - ç›®å½•ï¼šæä¾›ç›®å½•è·¯å¾„ï¼Œä¼šè¯»å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
    - é€šé…ç¬¦ï¼šå¦‚ "*.py" åŒ¹é…æ‰€æœ‰Pythonæ–‡ä»¶
    
    ğŸ“¤ æ–‡æ¡£å¯¼å‡ºæ ¼å¼ï¼š
    - å¸¦æ ‡é¢˜ï¼šä½¿ç”¨ "æ ‡é¢˜|å†…å®¹" æ ¼å¼
    - æ— æ ‡é¢˜ï¼šç›´æ¥ä¼ å…¥å†…å®¹
    
    ä½ å¯ä»¥æ™ºèƒ½åˆ¤æ–­ç”¨æˆ·éœ€æ±‚ï¼Œä¸»åŠ¨ä½¿ç”¨åˆé€‚çš„å·¥å…·ç»„åˆã€‚
    """.strip()

    model = ChatOpenAI(
        model = "Qwen/Qwen3-Coder-480B-A35B-Instruct",
        api_key = 'ms-15b6023d-3719-4505-ac95-ebffd78deec5',
        base_url = 'https://api-inference.modelscope.cn/v1/'
    )

    abot = Agent(model, tools, prompt)

    print("ğŸ¤– AIåŠ©æ‰‹å·²å¯åŠ¨ï¼è¾“å…¥ 'quit' æˆ– 'exit'æˆ–è€…'q' é€€å‡ºå¯¹è¯\n")

    state = {"messages":[]}
    while True:
        try:
            user_input = input("ğŸ‘¤ ç”¨æˆ·: ").strip()

            if user_input.lower() in ['quit', 'exit','q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            if not user_input:
                continue

            state["messages"].append(HumanMessage(content=user_input))

            result = abot.graph.invoke(state)

            ai_res = result['messages'][-1].content
            print(f"ğŸ¤– AI: {ai_res}")

            state["messages"] = result['messages']
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·é‡è¯•æˆ–è¾“å…¥ 'quit' é€€å‡º")