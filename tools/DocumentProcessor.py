#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡æ¡£å¤„ç†å™¨ - æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼çš„è§£æå’Œåˆ†å—
æ”¯æŒMeta-Chunkingç­–ç•¥çš„æ–‡æ¡£åˆ†å—
"""

import os
import re
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import pypdf
import docx
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    MarkdownTextSplitter,
    PythonCodeTextSplitter
)
from langchain.docstore.document import Document
from openai import APIConnectionError, APITimeoutError
# å¯¼å…¥æ–‡æœ¬è§„èŒƒåŒ–å™¨
from .text_normalizer import TextNormalizer
from .clean_think import clean_response
# å°è¯•å¯¼å…¥å¯é€‰ä¾èµ–
try:
    import torch
    import torch.nn.functional as F
    from transformers import AutoModelForCausalLM, AutoTokenizer
except ImportError:
    torch = None
    F = None
    AutoModelForCausalLM = None
    AutoTokenizer = None
# å°è¯•å¯¼å…¥ OpenAIï¼ˆç”¨äº API è°ƒç”¨æ¨¡å‹ï¼‰
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None
    
"""
é»˜è®¤çš„ API é…ç½®ï¼ˆå¯ç›´æ¥åœ¨æ­¤å¤„å¡«å…¥ä½ çš„å®é™…å‚æ•°ï¼Œæˆ–ç”¨ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
ä¼˜å…ˆçº§ï¼šå‡½æ•°å…¥å‚ > ç¯å¢ƒå˜é‡ > ä¸‹æ–¹é»˜è®¤å¸¸é‡
"""
DEFAULT_API_MODEL_NAME = os.getenv("OPENAI_MODEL_NAME") or "qwen3:1.7b"
DEFAULT_API_BASE_URL = os.getenv("OPENAI_BASE_URL") or "http://localhost:11434/v1"
DEFAULT_API_KEY = os.getenv("OPENAI_API_KEY") or "ollama"
DEFAULT_API_STREAMING = (os.getenv("OPENAI_STREAMING") or "True").lower() in {"1", "True", "yes"}

def load_qwen_model(model_name="qwen3:1.7b", max_retries=3):
    """
    åŠ è½½Qwen3-1.7Bæ¨¡å‹å’Œåˆ†è¯å™¨
    
    Args:
        model_name: æ¨¡å‹åç§°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        tuple: (model, tokenizer) æˆ– (None, None) å¦‚æœåŠ è½½å¤±è´¥
    """
    import time
    
    for attempt in range(max_retries):
        try:
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ {model_name} (å°è¯• {attempt + 1}/{max_retries})...")
            
            # é€šè¿‡ModelScopeåŠ è½½æ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                trust_remote_code=True,
                use_fast=False  # ä½¿ç”¨æ…¢é€Ÿåˆ†è¯å™¨ä»¥æé«˜å…¼å®¹æ€§
            )
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡
                torch_dtype=torch.float16 if torch else None  # ä½¿ç”¨float16ä»¥èŠ‚çœå†…å­˜
            )
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            if model:
                model.eval()
                
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
            return model, tokenizer
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                print(f"ç­‰å¾…5ç§’åé‡è¯•...")
                time.sleep(5)
            else:
                print("å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    
    print("ğŸ’¡ å°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬çš„Meta-Chunkingç­–ç•¥")
    return None, None

def create_api_client(api_key: Optional[str] = None,
                      base_url: Optional[str] = None,
                      model_name: Optional[str] = None):
    """
    åˆ›å»ºç”¨äºé€šè¿‡ API è°ƒç”¨å¤§æ¨¡å‹çš„å®¢æˆ·ç«¯ï¼ˆåŸºäº openai SDKï¼‰ã€‚

    Args:
        api_key: API å¯†é’¥
        base_url: API åŸºç¡€åœ°å€ï¼ˆå¦‚ ModelScope: https://api-inference.modelscope.cn/v1ï¼‰
        model_name: æ¨¡å‹åç§°ï¼ˆæœåŠ¡ç«¯å¯è§£æçš„æ ‡è¯†ï¼‰

    Returns:
        tuple: (client, model_name) æˆ– (None, None) å¦‚æœåˆ›å»ºå¤±è´¥æˆ– openai æœªå®‰è£…
    """
    if OpenAI is None:
        print("è­¦å‘Š: openai SDK ä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»º API å®¢æˆ·ç«¯")
        return None, None
    try:
        # ä½¿ç”¨ä¼˜å…ˆçº§ï¼šå…¥å‚ > ç¯å¢ƒå˜é‡/é»˜è®¤å¸¸é‡
        resolved_api_key = api_key or DEFAULT_API_KEY
        resolved_base_url = base_url or DEFAULT_API_BASE_URL
        resolved_model_name = model_name or DEFAULT_API_MODEL_NAME

        client = OpenAI(api_key=resolved_api_key, base_url=resolved_base_url)
        return client, resolved_model_name
    except Exception as e:
        print(f"âŒ åˆ›å»º API å®¢æˆ·ç«¯å¤±è´¥: {e}")
        return None, None

class MetaChunking:
    """Meta-Chunkingç­–ç•¥å®ç°ç±»"""
    
    def __init__(self, model=None, tokenizer=None, api_client=None, api_model: Optional[str] = None, api_streaming: Optional[bool] = None):
        self.model = model
        self.tokenizer = tokenizer
        self.api_client = api_client
        self.api_model = api_model
        # è‹¥æœªæ˜¾å¼ä¼ å…¥ï¼Œåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        self.api_streaming = DEFAULT_API_STREAMING if api_streaming is None else api_streaming
        
    def get_prob_subtract(self, sentence1, sentence2, language):
        """è®¡ç®—ä¸¤ä¸ªå¥å­çš„æ¦‚ç‡å·®"""
        try:
            # ä¼˜å…ˆä½¿ç”¨ API è°ƒç”¨åšå†³ç­–
            if self.api_client and self.api_model:
                print("[MetaChunking][API] ä½¿ç”¨ API åšç›¸é‚»å¥å¯¹å†³ç­–")
                if language == 'zh':
                    query = '''è¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†å—ä»»åŠ¡.ä½ æ˜¯ä¸€ä½æ–‡æœ¬åˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®æä¾›çš„å¥å­çš„é€»è¾‘ç»“æ„å’Œè¯­ä¹‰å†…å®¹ï¼Œä»ä¸‹é¢ä¸¤ç§æ–¹æ¡ˆä¸­é€‰æ‹©ä¸€ç§åˆ†å—æ–¹å¼ï¼š
                    1. å°†"{}"åˆ†å‰²æˆ"{}"ä¸"{}"ä¸¤éƒ¨åˆ†ï¼›
                    2. å°†"{}"ä¸è¿›è¡Œåˆ†å‰²ï¼Œä¿æŒåŸå½¢å¼ï¼›
                    åªå›ç­”æ•°å­—1æˆ–2,ä¸è¦æä¾›å…¶ä»–è§£é‡Šã€‚/no_think'''.format(sentence1 + sentence2, sentence1, sentence2, sentence1 + sentence2)
                    prompt = "You are a helpful assistant.\n\n{}\n\nAssistant:".format(query)
                else:
                    query = '''This is a text chunking task. You are a text analysis expert. Please choose one of the following two options based on the logical structure and semantic content of the provided sentence:
                    1. Split "{}" into "{}" and "{}" two parts;
                    2. Keep "{}" unsplit in its original form;
                    Answer with 1 or 2 only./no_think'''.format(sentence1 + ' ' + sentence2, sentence1, sentence2, sentence1 + ' ' + sentence2)
                    prompt = "You are a helpful assistant.\n\n{}\n\nAssistant:".format(query)

                try:
                    resp = self.api_client.chat.completions.create(
                        model=self.api_model,
                        messages=[
                            {"role": "system", "content": "You are a helpful assistant."},
                            {"role": "user", "content": prompt},
                        ],
                        temperature=0,
                        stream=True,
                        timeout=8,   # â±ï¸ è®¾ç½®è¶…æ—¶æ—¶é—´ 8 ç§’
                        max_tokens=10
                    )
                    if True:
                        # æµå¼æ‹¼æ¥å†…å®¹ï¼ˆå…¼å®¹ä¸åŒæä¾›æ–¹å­—æ®µï¼‰
                        parts = []
                        for ch in resp:
                            try:
                                delta = ch.choices[0].delta
                                seg = getattr(delta, 'content', None)
                                if seg is None and isinstance(delta, dict):
                                    seg = delta.get('content')
                                if seg:
                                    parts.append(seg)
                            except Exception:
                                # å…¼å®¹éƒ¨åˆ†æä¾›æ–¹ä½¿ç”¨ message.content
                                try:
                                    seg2 = ch.choices[0].message.content
                                    if seg2:
                                        parts.append(seg2)
                                except Exception:
                                    pass
                        content = ("".join(parts)).strip()
                        content = clean_response(content)
                    
                    # è§„èŒƒåŒ–ï¼šåªå–ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸­çš„ 1/2
                    decision = '1' if '1' in content[:3] and '2' not in content[:3] else (
                        '2' if '2' in content[:3] else content[:1]
                    )
                    if decision == '2':
                        # é€‰æ‹©ä¸åˆ†å‰² -> è¿”å›æ­£å€¼ï¼ˆ> threshold è§†ä¸ºä¸åˆ†å‰²ï¼‰
                        print(f"[MetaChunking][API] å›å¤: {content!r} -> å†³ç­–=2(ä¸åˆ†å‰²) | è¿”å›åˆ†æ•°=1")
                        return 1
                    elif decision == '1':
                        # é€‰æ‹©åˆ†å‰² -> è¿”å›è´Ÿå€¼ï¼ˆ<= threshold è§†ä¸ºåˆ†å‰²ï¼‰
                        print(f"[MetaChunking][API] å›å¤: {content!r} -> å†³ç­–=1(åˆ†å‰²) | è¿”å›åˆ†æ•°=-1")
                        return -1
                    else:
                        # æ— æ³•è§£æï¼Œè¿”å›ä¸­æ€§
                        print(f"[MetaChunking][API] æ— æ³•è§£æå›å¤: {content!r} | è¿”å›åˆ†æ•°=0")
                        return 0
                except (APITimeoutError, APIConnectionError) as e:
                    print(f"âŒ API è¯·æ±‚è¶…æ—¶/è¿æ¥é”™è¯¯: {e}")
                    return 0  # å›é€€ï¼Œä¸å½±å“ä¸»æµç¨‹
                except Exception as e:
                    print(f"API å†³ç­–å¤±è´¥ï¼Œå›é€€æœ¬åœ°/ç®€åŒ–é€»è¾‘: {e}")
                    # å›é€€åˆ°æœ¬åœ°é€»è¾‘

            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æœ¬åœ°æ¨¡å‹å’Œåˆ†è¯å™¨
            if not self.model or not self.tokenizer:
                print("[MetaChunking][Local] æœ¬åœ°æ¨¡å‹æˆ–åˆ†è¯å™¨ä¸å¯ç”¨ï¼Œè¿”å›åˆ†æ•°0")
                return 0  # è¿”å›é»˜è®¤å€¼
            
            if language == 'zh':
                query = '''è¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†å—ä»»åŠ¡.ä½ æ˜¯ä¸€ä½æ–‡æœ¬åˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®æä¾›çš„å¥å­çš„é€»è¾‘ç»“æ„å’Œè¯­ä¹‰å†…å®¹ï¼Œä»ä¸‹é¢ä¸¤ç§æ–¹æ¡ˆä¸­é€‰æ‹©ä¸€ç§åˆ†å—æ–¹å¼ï¼š
                1. å°†"{}"åˆ†å‰²æˆ"{}"ä¸"{}"ä¸¤éƒ¨åˆ†ï¼›
                2. å°†"{}"ä¸è¿›è¡Œåˆ†å‰²ï¼Œä¿æŒåŸå½¢å¼ï¼›
                è¯·å›ç­”1æˆ–2ã€‚'''.format(sentence1 + sentence2, sentence1, sentence2, sentence1 + sentence2)
                prompt = "You are a helpful assistant.\n\n{}\n\nAssistant:".format(query)
            else:
                query = '''This is a text chunking task. You are a text analysis expert. Please choose one of the following two options based on the logical structure and semantic content of the provided sentence:
                1. Split "{}" into "{}" and "{}" two parts;
                2. Keep "{}" unsplit in its original form;
                Please answer 1 or 2.'''.format(sentence1 + ' ' + sentence2, sentence1, sentence2, sentence1 + ' ' + sentence2)
                prompt = "You are a helpful assistant.\n\n{}\n\nAssistant:".format(query)
            
            prompt_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.model.device)
            input_ids = prompt_ids
            output_ids = self.tokenizer.encode(['1','2'], return_tensors='pt').to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model(input_ids)
                next_token_logits = outputs.logits[:, -1, :]
                token_probs = F.softmax(next_token_logits, dim=-1)
            
            next_token_id_0 = output_ids[:, 0].unsqueeze(0)
            next_token_prob_0 = token_probs[:, next_token_id_0].item()      
            next_token_id_1 = output_ids[:, 1].unsqueeze(0)
            next_token_prob_1 = token_probs[:, next_token_id_1].item()  
            prob_subtract = next_token_prob_1 - next_token_prob_0
            
            return prob_subtract
        except Exception as e:
            print(f"è®¡ç®—æ¦‚ç‡å·®å¤±è´¥: {e}")
            return 0  # è¿”å›é»˜è®¤å€¼
    
    def _fallback_chunking(self, text):
        """å›é€€åˆ°é€’å½’å­—ç¬¦åˆ†å‰²"""
        try:
            # ä½¿ç”¨ç®€å•çš„å­—ç¬¦åˆ†å‰²ä½œä¸ºå›é€€æ–¹æ¡ˆ
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", ".", "!", "?", " ", ""]
            )
            return text_splitter.split_text(text)
        except Exception as e:
            print(f"å›é€€åˆ†å—ä¹Ÿå¤±è´¥äº†ï¼Œè¿”å›åŸå§‹æ–‡æœ¬: {e}")
            return [text]
    
    def split_text_by_punctuation(self, text, language): 
        """æ ¹æ®æ ‡ç‚¹ç¬¦å·åˆ†å‰²æ–‡æœ¬"""
        try:
            # æ£€æŸ¥jiebaæ˜¯å¦å¯ç”¨
            import jieba
            jieba_available = True
        except ImportError:
            jieba_available = False
            print("è­¦å‘Š: jiebaæ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ†å‰²")
        
        if language == 'zh': 
            # æ›´å¥å£®çš„ä¸­æ–‡æ–­å¥ï¼šæ”¯æŒæ›´å¤šæ ‡ç‚¹ä¸æ¢è¡Œ
            puncts = {"ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼š", "ï¼Œ", "ï¼"}
            if jieba_available:
                words = list(jieba.cut(text, cut_all=False))
                sentences = []
                buf = ""
                for w in words:
                    if w in puncts or w == "\n":
                        if buf.strip():
                            sentences.append(buf.strip() + (w if w != "\n" else ""))
                        buf = ""
                    else:
                        buf += w
                if buf.strip():
                    sentences.append(buf.strip())
                # å¦‚æœä»ç„¶è¿‡å°‘ï¼Œä½¿ç”¨æ­£åˆ™è¿›ä¸€æ­¥åˆ‡
                if len(sentences) <= 1:
                    import re
                    parts = re.split(r'[ã€‚ï¼ï¼Ÿï¼›ï¼šï¼Œï¼\n]+', text)
                    sentences = [s.strip() for s in parts if s.strip()]
                return sentences
            else:
                # çº¯æ­£åˆ™æ–­å¥
                import re
                parts = re.split(r'[ã€‚ï¼ï¼Ÿï¼›ï¼šï¼Œï¼\n]+', text)
                return [s.strip() for s in parts if s.strip()]
        else:
            try:
                from nltk.tokenize import sent_tokenize
                full_segments = sent_tokenize(text)
            except ImportError:
                # å¦‚æœnltkä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ†å‰²
                import re
                full_segments = re.split(r'[.!?]+', text)
            
            ret = []
            for item in full_segments:
                item_l = item.strip().split(' ')
                if len(item_l) > 512:
                    if len(item_l) > 1024:
                        item = ' '.join(item_l[:256]) + "..."
                    else:
                        item = ' '.join(item_l[:512]) + "..."
                ret.append(item)
            return ret
    
    def perplexity_chunking(self, text, threshold: float = 0.0, language: str = 'en', target_length: Optional[int] = None) -> List[str]:
        """åŸºäºå›°æƒ‘åº¦ï¼ˆPPLï¼‰çš„åˆ†å—ï¼š
        - å¥çº§åˆ‡åˆ†ï¼Œè®¡ç®— PPL åºåˆ— PPL(x1..xn) ç›¸å¯¹å‰æ–‡ä¸Šä¸‹æ–‡
        - è¯†åˆ«å±€éƒ¨æå°å€¼ä½œä¸ºå€™é€‰è¾¹ç•Œï¼›æ ¹æ®é˜ˆå€¼ä¸ç›®æ ‡é•¿åº¦è¿›è¡Œè¾¹ç•Œç­›é€‰
        - é•¿æ–‡æœ¬é‡‡ç”¨æ»‘åŠ¨çª—å£è¿‘ä¼¼ KV-cacheï¼šä»…ä¿ç•™æœ€è¿‘çª—å£ä¸Šä¸‹æ–‡
        """
        try:
            segments = self.split_text_by_punctuation(text, language)
            segments = [s for s in segments if s.strip()]
            if len(segments) <= 1:
                print(f"[Chunk][PPL] æ®µæ•°={len(segments)}ï¼Œç›´æ¥è¿”å›æ•´ä½“")
                return [text]

            def _calc_ppl_seq_with_transformers(seg_list: List[str]) -> List[float]:
                if not (self.model and self.tokenizer and torch is not None):
                    return []
                ppl_vals: List[float] = []
                context = ""
                # ç®€åŒ–æ»‘åŠ¨çª—å£ï¼ˆå­—ç¬¦çº§ï¼Œè¿‘ä¼¼ KV-cacheï¼‰ï¼š
                max_ctx_chars = 3000
                for i, sent in enumerate(seg_list):
                    if context:
                        prompt = context + (" " if language != 'zh' else "") + sent
                    else:
                        prompt = sent
                    input_ids = self.tokenizer(prompt, return_tensors='pt').input_ids.to(self.model.device)
                    with torch.no_grad():
                        out = self.model(input_ids)
                        logits = out.logits[:, :-1, :]
                        labels = input_ids[:, 1:]
                        nll = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1), reduction='mean')
                    ppl = float(torch.exp(nll).item()) if hasattr(torch, 'exp') else float(nll.item())
                    ppl_vals.append(ppl)
                    context = (context + (" " if language != 'zh' else "") + sent) if context else sent
                    if len(context) > max_ctx_chars:
                        context = context[-max_ctx_chars:]
                return ppl_vals

            def _calc_ppl_seq_proxy(seg_list: List[str]) -> List[float]:
                # æ— æ¨¡å‹æ—¶çš„ proxyï¼šç”¨é€†é•¿åº¦å’Œæ ‡ç‚¹å¯†åº¦æ··åˆï¼Œè¶Šéš¾é¢„æµ‹â†’è¶Šé«˜ PPL
                vals = []
                for s in seg_list:
                    ln = max(1, len(s))
                    punct = sum(c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›:ï¼š,.!?;â€¦' for c in s) + 1
                    vals.append((1000.0/ln) + (1.0/punct))
                return vals

            ppl_seq = _calc_ppl_seq_with_transformers(segments)
            if not ppl_seq:
                print("[Chunk][PPL] æ— å¯ç”¨æ¨¡å‹ï¼Œä½¿ç”¨ proxy PPL")
                ppl_seq = _calc_ppl_seq_proxy(segments)

            # å¯»æ‰¾å±€éƒ¨æå°å€¼ä½œä¸ºè¾¹ç•Œå€™é€‰
            minima = set()
            for i in range(1, len(ppl_seq)-1):
                if ppl_seq[i] <= ppl_seq[i-1] and ppl_seq[i] <= ppl_seq[i+1]:
                    minima.add(i)

            # æ ¹æ®é˜ˆå€¼ç­›é€‰ï¼ˆç›¸å¯¹å…¨å±€å‡å€¼/æ–¹å·®ï¼‰
            import statistics
            mean = statistics.mean(ppl_seq)
            stdev = statistics.pstdev(ppl_seq) if len(ppl_seq) > 1 else 0.0
            cut = mean - threshold * (stdev if stdev > 0 else 1.0)

            boundaries = []
            for i in sorted(minima):
                if ppl_seq[i] <= cut:
                    boundaries.append(i)

            # åŸºäºè¾¹ç•Œç”Ÿæˆå…ƒå—
            chunks: List[str] = []
            last = 0
            for b in boundaries:
                piece = segments[last:b+1]
                chunk = piece[0]
                for s in piece[1:]:
                    chunk = (chunk + (" " if language != 'zh' else "") + s)
                chunks.append(chunk)
                last = b+1
            if last < len(segments):
                rest = segments[last:]
                chunk = rest[0]
                for s in rest[1:]:
                    chunk = (chunk + (" " if language != 'zh' else "") + s)
                chunks.append(chunk)

            if not chunks:
                return [text]

            # åŠ¨æ€åˆå¹¶ä½¿å—é•¿æ¥è¿‘ç›®æ ‡é•¿åº¦
            if target_length and len(chunks) > 1:
                merged: List[str] = []
                acc = ""
                for ck in chunks:
                    if not acc:
                        acc = ck
                        continue
                    if len(acc) + len(ck) <= target_length:
                        acc = acc + ((" " if language != 'zh' else "") + ck)
                    else:
                        merged.append(acc)
                        acc = ck
                if acc:
                    merged.append(acc)
                print(f"[Chunk][PPL] ä¾æ®ç›®æ ‡é•¿åº¦={target_length} åˆå¹¶åå—æ•°={len(merged)}")
                return merged

            return chunks
        except Exception as e:
            print(f"å›°æƒ‘åº¦åˆ†å—å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤åˆ†å—: {e}")
            return self._fallback_chunking(text)
    
    def prob_subtract_chunking(self, text, threshold=0, language='en', target_length: Optional[int] = None) -> List[str]:
        """åŸºäºè¾¹ç¼˜é‡‡æ ·çš„åˆ†å—ï¼ˆMargin Samplingï¼‰ï¼š
        - å…ƒå— Xj ä¸ºå½“å‰ç´¯ç§¯å—ï¼Œxi ä¸ºä¸‹ä¸€å¥
        - ä½¿ç”¨ API å†³ç­–æ˜¯å¦å°† xi åˆå¹¶è¿› Xjï¼ˆscore ä¸åŠ¨æ€é˜ˆå€¼å¯¹æ¯”ï¼‰
        - å¯é€‰ target_lengthï¼šåœ¨åˆå¹¶æ—¶ä¸äº‹ååŠ¨æ€åˆå¹¶ï¼Œå°½é‡é è¿‘ç›®æ ‡å—é•¿
        """
        try:
            segments = self.split_text_by_punctuation(text, language)
            segments = [item for item in segments if item.strip()]

            if len(segments) <= 1:
                print(f"[Chunk][ProbSubtract] æ®µæ•°={len(segments)}ï¼Œç›´æ¥è¿”å›æ•´ä½“")
                return [text]

            # è‹¥æ—¢æ—  API ä¹Ÿæ— æœ¬åœ°æ¨¡å‹ï¼Œåˆ™å›é€€åˆ°ç®€åŒ–çš„é•¿åº¦é€»è¾‘
            if not ((self.api_client and self.api_model) or (self.model and self.tokenizer)):
                print(f"[Chunk][ProbSubtract] æ—  API/æœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨é•¿åº¦å›é€€é€»è¾‘ | æ®µæ•°={len(segments)}")
                chunks = []
                current_chunk = ""
                for segment in segments:
                    if current_chunk and len(current_chunk) + len(segment) > 800:
                        print(f"[Chunk][LenFallback] è§¦å‘åˆ‡åˆ† | å½“å‰å—é•¿åº¦={len(current_chunk)} | æ–°æ®µé•¿åº¦={len(segment)}")
                        chunks.append(current_chunk)
                        current_chunk = segment
                    else:
                        if language == 'zh':
                            current_chunk += segment
                        else:
                            current_chunk += (" " if current_chunk else "") + segment
                if current_chunk:
                    chunks.append(current_chunk)
                print(f"[Chunk][LenFallback] å®Œæˆ | å—æ•°={len(chunks)}")
                return chunks if chunks else [text]

            # ä½¿ç”¨ç›¸é‚»å¥å¯¹ + åŠ¨æ€é˜ˆå€¼ï¼ˆæœ€è¿‘5ä¸ªçš„å‡å€¼ï¼‰
            print(f"[Chunk][ProbSubtract] ä½¿ç”¨ç›¸é‚»å¥å¯¹ + åŠ¨æ€é˜ˆå€¼ | æ®µæ•°={len(segments)} | å†³ç­–æ¥æº={'API' if (self.api_client and self.api_model) else 'Local'}")
            chunks: List[str] = []
            tmp_chunk = ""
            scores: List[float] = []
            dynamic_threshold: float = 0.0

            for i, sentence in enumerate(segments):
                if tmp_chunk == "":
                    tmp_chunk = sentence
                    continue

                # ä½¿ç”¨å…ƒå— Xj(=tmp_chunk) vs å½“å‰å¥ xi(=sentence)
                score = self.get_prob_subtract(tmp_chunk, sentence, language)
                scores.append(score)

                # æ›´æ–°åŠ¨æ€é˜ˆå€¼ï¼ˆæœ€è¿‘5ä¸ªçš„å¹³å‡å€¼ï¼‰
                recent = scores[-5:]
                dynamic_threshold = sum(recent) / len(recent)

                # è‹¥æä¾›ç›®æ ‡å—é•¿ï¼Œä¼˜å…ˆé˜²æ­¢å—æ— é™å¢å¤§
                force_split = False
                if target_length and len(tmp_chunk) >= max(1, int(1.2 * target_length)):
                    force_split = True

                if not force_split and score > dynamic_threshold:
                    # åˆå¹¶åˆ°å½“å‰å— Xj
                    if language == 'zh':
                        tmp_chunk += sentence
                    else:
                        tmp_chunk += " " + sentence
                    print(f"[Chunk][Step i={i}] score={score:.4f} | thr={dynamic_threshold:.4f} -> åˆå¹¶")
                else:
                    # æ–°èµ·ä¸€ä¸ªå—
                    chunks.append(tmp_chunk)
                    print(f"[Chunk][Step i={i}] score={score:.4f} | thr={dynamic_threshold:.4f} | æ‰§è¡ŒåŠ¨ä½œ=åˆ‡åˆ† | å½“å‰å—æ•°={len(chunks)}")
                    tmp_chunk = sentence

            if tmp_chunk:
                chunks.append(tmp_chunk)
            print(f"[Chunk][ProbSubtract] å®Œæˆ | æœ€ç»ˆå—æ•°={len(chunks)}")

            # äº‹ååŠ¨æ€åˆå¹¶ï¼šå°½é‡é è¿‘ target_length
            if target_length and len(chunks) > 1:
                merged: List[str] = []
                acc = ""
                for ck in chunks:
                    if not acc:
                        acc = ck
                        continue
                    if len(acc) + len(ck) <= target_length:
                        if language == 'zh':
                            acc += ck
                        else:
                            acc += " " + ck
                    else:
                        merged.append(acc)
                        acc = ck
                if acc:
                    merged.append(acc)
                print(f"[Chunk][DynamicMerge] ä¾æ®ç›®æ ‡é•¿åº¦={target_length} åˆå¹¶åå—æ•°={len(merged)}")
                return merged if merged else chunks

            return chunks if chunks else [text]
        except Exception as e:
            print(f"æ¦‚ç‡å·®åˆ†å—å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤åˆ†å—: {e}")
            return self._fallback_chunking(text)
    
    def semantic_chunking(self, text, breakpoint_percentile_threshold=73, language='en'):
        """åŸºäºè¯­ä¹‰çš„åˆ†å—ç­–ç•¥ï¼ˆéœ€è¦llama_indexåº“ï¼‰"""
        try:
            # è¿™é‡Œæˆ‘ä»¬å®ç°ä¸€ä¸ªç®€åŒ–çš„è¯­ä¹‰åˆ†å—é€»è¾‘
            # å®é™…çš„è¯­ä¹‰åˆ†å—éœ€è¦ä½¿ç”¨åµŒå…¥æ¨¡å‹æ¥è¯†åˆ«è¯­ä¹‰è¾¹ç•Œ
            # ç”±äºä¾èµ–é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            segments = self.split_text_by_punctuation(text, language)
            segments = [item for item in segments if item.strip()]
            
            if len(segments) <= 1:
                return [text]
            
            # ç®€åŒ–çš„è¯­ä¹‰åˆ†å—ï¼šæ ¹æ®å¥å­é•¿åº¦å’Œæ ‡ç‚¹ç¬¦å·è¿›è¡Œåˆ†ç»„
            chunks = []
            current_chunk = ""
            
            for segment in segments:
                if len(current_chunk) + len(segment) > 1000:  # å‡è®¾æœ€å¤§å—å¤§å°ä¸º1000å­—ç¬¦
                    if current_chunk:
                        chunks.append(current_chunk)
                    current_chunk = segment
                else:
                    if language == 'zh':
                        current_chunk += segment
                    else:
                        current_chunk += " " + segment if current_chunk else segment
            
            if current_chunk:
                chunks.append(current_chunk)
            
            return chunks if chunks else [text]
        except Exception as e:
            print(f"è¯­ä¹‰åˆ†å—å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤åˆ†å—: {e}")
            return self._fallback_chunking(text)

class DocumentProcessor:
    """
    æ–‡æ¡£å¤„ç†å™¨ - æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼çš„è§£æå’Œåˆ†å—
    """
    
    def __init__(self, 
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200,
                 separators: Optional[List[str]] = None,
                 meta_chunking_strategy: Optional[str] = None,
                 meta_model = None,
                 meta_tokenizer = None,
                 api_client = None,
                 api_model: Optional[str] = None):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        Args:
            chunk_size: æ–‡æ¡£å—å¤§å°
            chunk_overlap: å—ä¹‹é—´çš„é‡å 
            separators: è‡ªå®šä¹‰åˆ†éš”ç¬¦
            meta_chunking_strategy: Meta-Chunkingç­–ç•¥ ("perplexity", "prob_subtract", "semantic", None)
            meta_model: ç”¨äºMeta-Chunkingçš„æ¨¡å‹
            meta_tokenizer: ç”¨äºMeta-Chunkingçš„åˆ†è¯å™¨
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        self.supported_formats = {
            '.txt': self._process_text,
            '.md': self._process_markdown,
            '.py': self._process_python,
            '.js': self._process_javascript,
            '.java': self._process_java,
            '.cpp': self._process_cpp,
            '.c': self._process_cpp,
            '.h': self._process_cpp,
            '.hpp': self._process_cpp,
            '.html': self._process_html,
            '.xml': self._process_xml,
            '.json': self._process_json,
            '.csv': self._process_csv,
            '.pdf': self._process_pdf,
            '.docx': self._process_docx,
            '.log': self._process_text,
            '.cfg': self._process_text,
            '.ini': self._process_text,
            '.yaml': self._process_text,
            '.yml': self._process_text,
        }
        
        # åˆå§‹åŒ–ä¸åŒç±»å‹çš„æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=separators or ["\n\n", "\n", " ", ""]
        )
        
        self.markdown_splitter = MarkdownTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        self.code_splitter = PythonCodeTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # åˆå§‹åŒ–Meta-Chunking
        self.meta_chunking_strategy = meta_chunking_strategy
        if meta_chunking_strategy:
            self.meta_chunking = MetaChunking(meta_model, meta_tokenizer, api_client=api_client, api_model=api_model)
        else:
            self.meta_chunking = None
        
        # åˆå§‹åŒ–æ–‡æœ¬è§„èŒƒåŒ–å™¨
        self.text_normalizer = TextNormalizer()
        
    def process_file(self, file_path: str) -> List[Document]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            List[Document]: å¤„ç†åçš„æ–‡æ¡£å—åˆ—è¡¨
        """
        try:
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # è·å–æ–‡ä»¶æ‰©å±•å
            file_ext = Path(file_path).suffix.lower()
            
            if file_ext not in self.supported_formats:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
            
            # è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
            file_stats = os.stat(file_path)
            base_metadata = {
                "source": os.path.abspath(file_path),
                "file_name": os.path.basename(file_path),
                "file_type": file_ext,
                "file_size": file_stats.st_size,
                "modified_time": file_stats.st_mtime
            }
            
            # è°ƒç”¨å¯¹åº”çš„å¤„ç†å‡½æ•°
            processor_func = self.supported_formats[file_ext]
            documents = processor_func(file_path, base_metadata)
            
            print(f"âœ… æˆåŠŸå¤„ç†æ–‡ä»¶: {os.path.basename(file_path)} - {len(documents)} ä¸ªå—")
            return documents
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []
    
    def process_directory(self, directory_path: str, 
                         recursive: bool = True,
                         file_pattern: Optional[str] = None) -> List[Document]:
        """
        å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰æ”¯æŒæ–‡ä»¶
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            recursive: æ˜¯å¦é€’å½’å¤„ç†å­ç›®å½•
            file_pattern: æ–‡ä»¶åæ¨¡å¼è¿‡æ»¤ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
            
        Returns:
            List[Document]: æ‰€æœ‰æ–‡æ¡£å—åˆ—è¡¨
        """
        all_documents = []
        
        try:
            directory = Path(directory_path)
            if not directory.exists() or not directory.is_dir():
                raise ValueError(f"ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {directory_path}")
            
            # è·å–æ–‡ä»¶åˆ—è¡¨
            if recursive:
                files = directory.rglob("*")
            else:
                files = directory.glob("*")
            
            processed_files = 0
            
            for file_path in files:
                if not file_path.is_file():
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                if file_path.suffix.lower() not in self.supported_formats:
                    continue
                
                # åº”ç”¨æ–‡ä»¶åæ¨¡å¼è¿‡æ»¤
                if file_pattern and not re.match(file_pattern, file_path.name):
                    continue
                
                # å¤„ç†æ–‡ä»¶
                documents = self.process_file(str(file_path))
                all_documents.extend(documents)
                processed_files += 1
            
            print(f"âœ… ç›®å½•å¤„ç†å®Œæˆ: {processed_files} ä¸ªæ–‡ä»¶, å…± {len(all_documents)} ä¸ªæ–‡æ¡£å—")
            return all_documents
            
        except Exception as e:
            print(f"âŒ å¤„ç†ç›®å½•å¤±è´¥ {directory_path}: {e}")
            return []
    
    def process_text(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[Document]:
        """
        ç›´æ¥å¤„ç†æ–‡æœ¬å†…å®¹
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            metadata: å…ƒæ•°æ®
            
        Returns:
            List[Document]: æ–‡æ¡£å—åˆ—è¡¨
        """
        try:
            if not text.strip():
                return []
            
            # æ–‡æœ¬è§„èŒƒåŒ–
            normalized_text = self.text_normalizer.normalize_text(text)
            
            # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
            if self.meta_chunking and self.meta_chunking_strategy:
                if self.meta_chunking_strategy == 'perplexity':
                    chunks = self.meta_chunking.perplexity_chunking(normalized_text)
                elif self.meta_chunking_strategy == 'prob_subtract':
                    chunks = self.meta_chunking.prob_subtract_chunking(normalized_text)
                elif self.meta_chunking_strategy == 'semantic':
                    chunks = self.meta_chunking.semantic_chunking(normalized_text)
                else:
                    chunks = self.text_splitter.split_text(normalized_text)
            else:
                chunks = self.text_splitter.split_text(normalized_text)
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            documents = []
            for i, chunk in enumerate(chunks):
                doc_metadata = metadata.copy() if metadata else {}
                doc_metadata.update({
                    "chunk_index": i,
                    "chunk_size": len(chunk)
                })
                
                documents.append(Document(
                    page_content=chunk,
                    metadata=doc_metadata
                ))
            
            return documents
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡æœ¬å¤±è´¥: {e}")
            return []
    
    def _process_text(self, file_path: str, base_metadata: Dict) -> List[Document]:
        """å¤„ç†çº¯æ–‡æœ¬æ–‡ä»¶"""
        encodings = ['utf-8', 'gbk', 'latin-1']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read()
                break
            except UnicodeDecodeError:
                continue
        else:
            raise ValueError(f"æ— æ³•è§£ç æ–‡ä»¶: {file_path}")
        
        # æ–‡æœ¬è§„èŒƒåŒ–
        normalized_content = self.text_normalizer.normalize_text(content)
        
        # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
        if self.meta_chunking and self.meta_chunking_strategy:
            if self.meta_chunking_strategy == 'perplexity':
                chunks = self.meta_chunking.perplexity_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'prob_subtract':
                chunks = self.meta_chunking.prob_subtract_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'semantic':
                chunks = self.meta_chunking.semantic_chunking(normalized_content)
            else:
                chunks = self.text_splitter.split_text(normalized_content)
        else:
            chunks = self.text_splitter.split_text(normalized_content)
        
        return self._create_documents(chunks, base_metadata)
    
    def _process_markdown(self, file_path: str, base_metadata: Dict) -> List[Document]:
        """å¤„ç†Markdownæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ–‡æœ¬è§„èŒƒåŒ–
        normalized_content = self.text_normalizer.normalize_text(content)
        
        # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
        if self.meta_chunking and self.meta_chunking_strategy:
            if self.meta_chunking_strategy == 'perplexity':
                chunks = self.meta_chunking.perplexity_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'prob_subtract':
                chunks = self.meta_chunking.prob_subtract_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'semantic':
                chunks = self.meta_chunking.semantic_chunking(normalized_content)
            else:
                chunks = self.markdown_splitter.split_text(normalized_content)
        else:
            chunks = self.markdown_splitter.split_text(normalized_content)
        
        return self._create_documents(chunks, base_metadata)
    
    def _process_python(self, file_path: str, base_metadata: Dict) -> List[Document]:
        """å¤„ç†Pythonä»£ç æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ–‡æœ¬è§„èŒƒåŒ–
        normalized_content = self.text_normalizer.normalize_text(content)
        
        # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
        if self.meta_chunking and self.meta_chunking_strategy:
            if self.meta_chunking_strategy == 'perplexity':
                chunks = self.meta_chunking.perplexity_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'prob_subtract':
                chunks = self.meta_chunking.prob_subtract_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'semantic':
                chunks = self.meta_chunking.semantic_chunking(normalized_content)
            else:
                chunks = self.code_splitter.split_text(normalized_content)
        else:
            chunks = self.code_splitter.split_text(normalized_content)
        
        return self._create_documents(chunks, base_metadata)
    
    def _process_javascript(self, file_path: str, base_metadata: Dict) -> List[Document]:
        """å¤„ç†JavaScriptæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ–‡æœ¬è§„èŒƒåŒ–
        normalized_content = self.text_normalizer.normalize_text(content)
        
        # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
        if self.meta_chunking and self.meta_chunking_strategy:
            if self.meta_chunking_strategy == 'perplexity':
                chunks = self.meta_chunking.perplexity_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'prob_subtract':
                chunks = self.meta_chunking.prob_subtract_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'semantic':
                chunks = self.meta_chunking.semantic_chunking(normalized_content)
            else:
                chunks = self.text_splitter.split_text(normalized_content)
        else:
            chunks = self.text_splitter.split_text(normalized_content)
        
        return self._create_documents(chunks, base_metadata)
    
    def _process_java(self, file_path: str, base_metadata: Dict) -> List[Document]:
        """å¤„ç†Javaæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ–‡æœ¬è§„èŒƒåŒ–
        normalized_content = self.text_normalizer.normalize_text(content)
        
        # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
        if self.meta_chunking and self.meta_chunking_strategy:
            if self.meta_chunking_strategy == 'perplexity':
                chunks = self.meta_chunking.perplexity_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'prob_subtract':
                chunks = self.meta_chunking.prob_subtract_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'semantic':
                chunks = self.meta_chunking.semantic_chunking(normalized_content)
            else:
                chunks = self.text_splitter.split_text(normalized_content)
        else:
            chunks = self.text_splitter.split_text(normalized_content)
        
        return self._create_documents(chunks, base_metadata)
    
    def _process_cpp(self, file_path: str, base_metadata: Dict) -> List[Document]:
        """å¤„ç†C/C++æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ–‡æœ¬è§„èŒƒåŒ–
        normalized_content = self.text_normalizer.normalize_text(content)
        
        # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
        if self.meta_chunking and self.meta_chunking_strategy:
            if self.meta_chunking_strategy == 'perplexity':
                chunks = self.meta_chunking.perplexity_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'prob_subtract':
                chunks = self.meta_chunking.prob_subtract_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'semantic':
                chunks = self.meta_chunking.semantic_chunking(normalized_content)
            else:
                chunks = self.text_splitter.split_text(normalized_content)
        else:
            chunks = self.text_splitter.split_text(normalized_content)
        
        return self._create_documents(chunks, base_metadata)
    
    def _process_html(self, file_path: str, base_metadata: Dict) -> List[Document]:
        """å¤„ç†HTMLæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ç®€å•å»é™¤HTMLæ ‡ç­¾ï¼ˆå¯ä»¥åç»­ä½¿ç”¨BeautifulSoupæ”¹è¿›ï¼‰
        import re
        content = re.sub(r'<[^>]+>', '', content)
        content = re.sub(r'\s+', ' ', content).strip()
        
        # æ–‡æœ¬è§„èŒƒåŒ–
        normalized_content = self.text_normalizer.normalize_text(content)
        
        # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
        if self.meta_chunking and self.meta_chunking_strategy:
            if self.meta_chunking_strategy == 'perplexity':
                chunks = self.meta_chunking.perplexity_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'prob_subtract':
                chunks = self.meta_chunking.prob_subtract_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'semantic':
                chunks = self.meta_chunking.semantic_chunking(normalized_content)
            else:
                chunks = self.text_splitter.split_text(normalized_content)
        else:
            chunks = self.text_splitter.split_text(normalized_content)
        
        return self._create_documents(chunks, base_metadata)
    
    def _process_xml(self, file_path: str, base_metadata: Dict) -> List[Document]:
        """å¤„ç†XMLæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ–‡æœ¬è§„èŒƒåŒ–
        normalized_content = self.text_normalizer.normalize_text(content)
        
        # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
        if self.meta_chunking and self.meta_chunking_strategy:
            if self.meta_chunking_strategy == 'perplexity':
                chunks = self.meta_chunking.perplexity_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'prob_subtract':
                chunks = self.meta_chunking.prob_subtract_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'semantic':
                chunks = self.meta_chunking.semantic_chunking(normalized_content)
            else:
                chunks = self.text_splitter.split_text(normalized_content)
        else:
            chunks = self.text_splitter.split_text(normalized_content)
        
        return self._create_documents(chunks, base_metadata)
    
    def _process_json(self, file_path: str, base_metadata: Dict) -> List[Document]:
        """å¤„ç†JSONæ–‡ä»¶"""
        import json
        
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                content = json.dumps(data, ensure_ascii=False, indent=2)
            except json.JSONDecodeError:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½œä¸ºæ™®é€šæ–‡æœ¬å¤„ç†
                f.seek(0)
                content = f.read()
        
        # æ–‡æœ¬è§„èŒƒåŒ–
        normalized_content = self.text_normalizer.normalize_text(content)
        
        # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
        if self.meta_chunking and self.meta_chunking_strategy:
            if self.meta_chunking_strategy == 'perplexity':
                chunks = self.meta_chunking.perplexity_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'prob_subtract':
                chunks = self.meta_chunking.prob_subtract_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'semantic':
                chunks = self.meta_chunking.semantic_chunking(normalized_content)
            else:
                chunks = self.text_splitter.split_text(normalized_content)
        else:
            chunks = self.text_splitter.split_text(normalized_content)
        
        return self._create_documents(chunks, base_metadata)
    
    def _process_csv(self, file_path: str, base_metadata: Dict) -> List[Document]:
        """å¤„ç†CSVæ–‡ä»¶"""
        import csv
        
        content_lines = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                reader = csv.reader(f)
                for row in reader:
                    content_lines.append(",".join(row))
            except Exception:
                # å¦‚æœCSVè§£æå¤±è´¥ï¼Œä½œä¸ºæ™®é€šæ–‡æœ¬å¤„ç†
                f.seek(0)
                content_lines = f.readlines()
        
        content = "\n".join(content_lines)
        
        # æ–‡æœ¬è§„èŒƒåŒ–
        normalized_content = self.text_normalizer.normalize_text(content)
        
        # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
        if self.meta_chunking and self.meta_chunking_strategy:
            if self.meta_chunking_strategy == 'perplexity':
                chunks = self.meta_chunking.perplexity_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'prob_subtract':
                chunks = self.meta_chunking.prob_subtract_chunking(normalized_content)
            elif self.meta_chunking_strategy == 'semantic':
                chunks = self.meta_chunking.semantic_chunking(normalized_content)
            else:
                chunks = self.text_splitter.split_text(normalized_content)
        else:
            chunks = self.text_splitter.split_text(normalized_content)
        
        return self._create_documents(chunks, base_metadata)
    
    def _process_pdf(self, file_path: str, base_metadata: Dict) -> List[Document]:
        """å¤„ç†PDFæ–‡ä»¶"""
        try:
            with open(file_path, 'rb') as f:
                reader = pypdf.PdfReader(f)
                content = ""
                
                for page_num, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    if page_text.strip():
                        content += f"\n--- ç¬¬ {page_num + 1} é¡µ ---\n"
                        content += page_text + "\n"
            
            if not content.strip():
                raise ValueError("PDFæ–‡ä»¶æ— æ³•æå–æ–‡æœ¬å†…å®¹")
            
            # æ–‡æœ¬è§„èŒƒåŒ–
            normalized_content = self.text_normalizer.normalize_text(content)
            
            # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
            if self.meta_chunking and self.meta_chunking_strategy:
                if self.meta_chunking_strategy == 'perplexity':
                    chunks = self.meta_chunking.perplexity_chunking(normalized_content)
                elif self.meta_chunking_strategy == 'prob_subtract':
                    chunks = self.meta_chunking.prob_subtract_chunking(normalized_content)
                elif self.meta_chunking_strategy == 'semantic':
                    chunks = self.meta_chunking.semantic_chunking(normalized_content)
                else:
                    chunks = self.text_splitter.split_text(normalized_content)
            else:
                chunks = self.text_splitter.split_text(normalized_content)
            
            return self._create_documents(chunks, base_metadata)
            
        except Exception as e:
            print(f"âŒ PDFå¤„ç†å¤±è´¥: {e}")
            return []
    
    def _process_docx(self, file_path: str, base_metadata: Dict) -> List[Document]:
        """å¤„ç†Wordæ–‡æ¡£"""
        try:
            doc = docx.Document(file_path)
            content = ""
            
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    content += paragraph.text + "\n"
            
            # å¤„ç†è¡¨æ ¼
            for table in doc.tables:
                for row in table.rows:
                    row_text = " | ".join([cell.text.strip() for cell in row.cells])
                    if row_text.strip():
                        content += row_text + "\n"
            
            if not content.strip():
                raise ValueError("Wordæ–‡æ¡£æ— æ³•æå–æ–‡æœ¬å†…å®¹")
            
            # æ–‡æœ¬è§„èŒƒåŒ–
            normalized_content = self.text_normalizer.normalize_text(content)
            
            # ä½¿ç”¨Meta-Chunkingæˆ–é»˜è®¤åˆ†å‰²å™¨
            if self.meta_chunking and self.meta_chunking_strategy:
                if self.meta_chunking_strategy == 'perplexity':
                    chunks = self.meta_chunking.perplexity_chunking(normalized_content)
                elif self.meta_chunking_strategy == 'prob_subtract':
                    chunks = self.meta_chunking.prob_subtract_chunking(normalized_content)
                elif self.meta_chunking_strategy == 'semantic':
                    chunks = self.meta_chunking.semantic_chunking(normalized_content)
                else:
                    chunks = self.text_splitter.split_text(normalized_content)
            else:
                chunks = self.text_splitter.split_text(normalized_content)
            
            return self._create_documents(chunks, base_metadata)
            
        except Exception as e:
            print(f"âŒ Wordæ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            return []
    
    def _create_documents(self, chunks: List[str], base_metadata: Dict) -> List[Document]:
        """åˆ›å»ºæ–‡æ¡£å¯¹è±¡åˆ—è¡¨"""
        documents = []
        
        for i, chunk in enumerate(chunks):
            if not chunk.strip():
                continue
            
            doc_metadata = base_metadata.copy()
            doc_metadata.update({
                "chunk_index": i,
                "chunk_size": len(chunk),
                "total_chunks": len(chunks)
            })
            
            documents.append(Document(
                page_content=chunk,
                metadata=doc_metadata
            ))
        
        return documents
    
    def get_supported_formats(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶æ ¼å¼åˆ—è¡¨"""
        return list(self.supported_formats.keys())
    
    def estimate_chunks(self, text_length: int) -> int:
        """ä¼°ç®—æ–‡æœ¬å—æ•°é‡"""
        return max(1, text_length // (self.chunk_size - self.chunk_overlap))


def create_document_processor(chunk_size: int = 1000,
                            chunk_overlap: int = 200,
                            meta_chunking_strategy: Optional[str] = None,
                            meta_model = None,
                            meta_tokenizer = None,
                            api_client = None,
                            api_model: Optional[str] = None) -> DocumentProcessor:
    """
    åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨å®ä¾‹
    
    Args:
        chunk_size: æ–‡æ¡£å—å¤§å°
        chunk_overlap: å—é‡å å¤§å°
        meta_chunking_strategy: Meta-Chunkingç­–ç•¥
        meta_model: ç”¨äºMeta-Chunkingçš„æ¨¡å‹
        meta_tokenizer: ç”¨äºMeta-Chunkingçš„åˆ†è¯å™¨
        
    Returns:
        DocumentProcessor: æ–‡æ¡£å¤„ç†å™¨å®ä¾‹
    """
    return DocumentProcessor(
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap,
        meta_chunking_strategy=meta_chunking_strategy,
        meta_model=meta_model,
        meta_tokenizer=meta_tokenizer,
        api_client=api_client,
        api_model=api_model
    )


# æµ‹è¯•ä¸»å‡½æ•°
def main():
    # æµ‹è¯•æ–‡æœ¬
    test_text = (
        "è¿™æ˜¯ç¬¬ä¸€å¥ä¸­æ–‡æµ‹è¯•æ–‡æœ¬ã€‚"
        "è¿™æ˜¯ç¬¬äºŒå¥ï¼Œç”¨äºæµ‹è¯•åˆ†å—æ•ˆæœï¼"
        "ç¬¬ä¸‰å¥ç»§ç»­æµ‹è¯•ï¼Ÿ"
        "ç¬¬å››å¥ç”¨äºæ£€éªŒMeta-Chunkingæ˜¯å¦æ­£å¸¸å·¥ä½œã€‚"
    )

    print("===== 1. æµ‹è¯•åŠ è½½æœ¬åœ°æ¨¡å‹ =====")
    model, tokenizer = load_qwen_model()
    if model and tokenizer:
        print("âœ… æœ¬åœ°æ¨¡å‹å¯ç”¨")
    else:
        print("âš ï¸ æœ¬åœ°æ¨¡å‹ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨APIæˆ–å›é€€é€»è¾‘")

    print("\n===== 2. æµ‹è¯•åˆ›å»ºAPIå®¢æˆ·ç«¯ =====")
    api_client, api_model_name = create_api_client()
    if api_client:
        print(f"âœ… APIå®¢æˆ·ç«¯å¯ç”¨ï¼Œæ¨¡å‹: {api_model_name}")
    else:
        print("âš ï¸ APIå®¢æˆ·ç«¯ä¸å¯ç”¨")

    print("\n===== 3. æµ‹è¯•MetaChunkingåˆ†å— =====")
    # åˆå§‹åŒ–MetaChunking
    meta_chunker = MetaChunking(
        model=model,
        tokenizer=tokenizer,
        api_client=api_client,
        api_model=api_model_name
    )

    # æµ‹è¯•prob_subtract_chunking
    chunks = meta_chunker.prob_subtract_chunking(test_text, language='zh', target_length=50)
    print(f"åˆ†å—ç»“æœ ({len(chunks)} å—):")
    for i, c in enumerate(chunks, 1):
        print(f"[å—{i}] {c}")

    print("\n===== 4. æµ‹è¯•DocumentProcessorå¤„ç†æ–‡æœ¬ =====")
    doc_processor = DocumentProcessor(
        chunk_size=50,
        chunk_overlap=10,
        meta_chunking_strategy='prob_subtract',
        meta_model=model,
        meta_tokenizer=tokenizer,
        api_client=api_client,
        api_model=api_model_name
    )
    documents = doc_processor.process_text(test_text)
    print(f"å¤„ç†åçš„Documentå¯¹è±¡æ•°é‡: {len(documents)}")
    for i, doc in enumerate(documents, 1):
        print(f"[Document {i}] {doc.page_content}")

if __name__ == "__main__":
    main()
    
