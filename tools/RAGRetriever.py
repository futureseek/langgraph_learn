import os
import json
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from langchain.tools import Tool, StructuredTool
from langchain.schema import Document
from langchain_core.messages import HumanMessage, SystemMessage
from pydantic import BaseModel, Field

from .VectorDatabase import VectorDatabase, create_vector_database
from .DocumentProcessor import DocumentProcessor, create_document_processor, create_api_client


class RAGRetriever:
    """
    RAGæ£€ç´¢å™¨ - å®ç°å®Œæ•´çš„RAGé—®ç­”åŠŸèƒ½
    """
    
    def __init__(self, 
                 vector_db: Optional[VectorDatabase] = None,
                 doc_processor: Optional[DocumentProcessor] = None,
                 model=None):
        """
        åˆå§‹åŒ–RAGæ£€ç´¢å™¨
        
        Args:
            vector_db: å‘é‡æ•°æ®åº“å®ä¾‹
            doc_processor: æ–‡æ¡£å¤„ç†å™¨å®ä¾‹
            model: è¯­è¨€æ¨¡å‹å®ä¾‹
        """
        self.vector_db = vector_db or create_vector_database()
        self.doc_processor = doc_processor or create_document_processor()
        self.model = model
        
        # RAGé…ç½®
        self.config = {
            "retrieval_k": 4,  # æ£€ç´¢æ–‡æ¡£æ•°é‡
            "score_threshold": 0.3,  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé™ä½ä»¥æé«˜å¬å›ç‡ï¼‰
            "max_context_length": 4000,  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            "enable_reranking": True,  # æ˜¯å¦å¯ç”¨é‡æ–°æ’åº
        }
    
    def add_document_to_knowledge_base(self, file_path: str) -> Dict[str, Any]:
        """
        å°†æ–‡æ¡£æ·»åŠ åˆ°çŸ¥è¯†åº“
        
        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: æ·»åŠ ç»“æœ
        """
        try:
            if not os.path.exists(file_path):
                return {
                    "success": False,
                    "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}",
                    "file_path": file_path
                }
            
            # å¤„ç†æ–‡æ¡£
            print(f"ğŸ“„ æ­£åœ¨å¤„ç†æ–‡æ¡£: {os.path.basename(file_path)}")
            documents = self.doc_processor.process_file(file_path)
            
            if not documents:
                return {
                    "success": False,
                    "error": "æ–‡æ¡£å¤„ç†å¤±è´¥æˆ–æ–‡æ¡£ä¸ºç©º",
                    "file_path": file_path
                }
            
            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            success = self.vector_db.add_documents(documents, file_path)
            
            if success:
                stats = self.vector_db.get_stats()
                return {
                    "success": True,
                    "message": f"æˆåŠŸæ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“",
                    "file_path": file_path,
                    "chunks_added": len(documents),
                    "total_chunks": stats.get("total_chunks", 0),
                    "total_documents": stats.get("total_documents", 0)
                }
            else:
                return {
                    "success": False,
                    "error": "æ·»åŠ åˆ°å‘é‡æ•°æ®åº“å¤±è´¥",
                    "file_path": file_path
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}",
                "file_path": file_path
            }
    
    def add_directory_to_knowledge_base(self, directory_path: str, 
                                      recursive: bool = True) -> Dict[str, Any]:
        """
        å°†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£æ·»åŠ åˆ°çŸ¥è¯†åº“
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            recursive: æ˜¯å¦é€’å½’å¤„ç†å­ç›®å½•
            
        Returns:
            Dict: æ·»åŠ ç»“æœ
        """
        try:
            if not os.path.isdir(directory_path):
                return {
                    "success": False,
                    "error": f"ç›®å½•ä¸å­˜åœ¨: {directory_path}"
                }
            
            print(f"ğŸ“ æ­£åœ¨å¤„ç†ç›®å½•: {directory_path}")
            documents = self.doc_processor.process_directory(directory_path, recursive)
            
            if not documents:
                return {
                    "success": False,
                    "error": "ç›®å½•ä¸­æ²¡æœ‰å¯å¤„ç†çš„æ–‡æ¡£",
                    "directory_path": directory_path
                }
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„å¹¶é€ä¸ªæ·»åŠ 
            file_groups = {}
            for doc in documents:
                file_path = doc.metadata.get("source", "unknown")
                if file_path not in file_groups:
                    file_groups[file_path] = []
                file_groups[file_path].append(doc)
            
            total_added = 0
            processed_files = 0
            errors = []
            
            for file_path, file_docs in file_groups.items():
                try:
                    success = self.vector_db.add_documents(file_docs, file_path)
                    if success:
                        total_added += len(file_docs)
                        processed_files += 1
                    else:
                        errors.append(f"æ·»åŠ å¤±è´¥: {os.path.basename(file_path)}")
                except Exception as e:
                    errors.append(f"å¤„ç†é”™è¯¯: {os.path.basename(file_path)} - {str(e)}")
            
            stats = self.vector_db.get_stats()
            
            return {
                "success": processed_files > 0,
                "message": f"ç›®å½•å¤„ç†å®Œæˆ",
                "directory_path": directory_path,
                "processed_files": processed_files,
                "total_chunks_added": total_added,
                "total_chunks": stats.get("total_chunks", 0),
                "total_documents": stats.get("total_documents", 0),
                "errors": errors
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"å¤„ç†ç›®å½•æ—¶å‡ºé”™: {str(e)}",
                "directory_path": directory_path
            }
    
    def retrieve_relevant_documents(self, query: str, k: Optional[int] = None) -> List[Document]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ–‡æ¡£æ•°é‡
            
        Returns:
            List[Document]: ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        try:
            k = k or self.config["retrieval_k"]
            
            # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
            results = self.vector_db.similarity_search_with_score(query, k=k * 2)  # è·å–æ›´å¤šå€™é€‰
            
            # è¿‡æ»¤ä½åˆ†æ–‡æ¡£
            filtered_results = []
            for doc, score in results:
                if score >= self.config["score_threshold"]:
                    doc.metadata["relevance_score"] = score
                    filtered_results.append(doc)
            
            # é‡æ–°æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config["enable_reranking"] and len(filtered_results) > k:
                filtered_results = self._rerank_documents(query, filtered_results)
            
            return filtered_results[:k]
            
        except Exception as e:
            print(f"âŒ æ£€ç´¢æ–‡æ¡£å¤±è´¥: {e}")
            return []
    
    def _rerank_documents(self, query: str, documents: List[Document]) -> List[Document]:
        """
        é‡æ–°æ’åºæ–‡æ¡£ï¼ˆç®€å•çš„åŸºäºå…³é”®è¯åŒ¹é…çš„æ’åºï¼‰
        """
        try:
            query_words = set(query.lower().split())
            
            def calculate_keyword_score(doc: Document) -> float:
                content_words = set(doc.page_content.lower().split())
                intersection = query_words.intersection(content_words)
                return len(intersection) / len(query_words) if query_words else 0
            
            # ç»“åˆç›¸ä¼¼åº¦åˆ†æ•°å’Œå…³é”®è¯åˆ†æ•°
            scored_docs = []
            for doc in documents:
                similarity_score = doc.metadata.get("relevance_score", 0)
                keyword_score = calculate_keyword_score(doc)
                combined_score = 0.7 * similarity_score + 0.3 * keyword_score
                
                doc.metadata["combined_score"] = combined_score
                scored_docs.append((doc, combined_score))
            
            # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            return [doc for doc, _ in scored_docs]
            
        except Exception as e:
            print(f"âš ï¸ é‡æ–°æ’åºå¤±è´¥: {e}")
            return documents
    
    def answer_question(self, question: str, context_docs: Optional[List[Document]] = None) -> Dict[str, Any]:
        """
        åŸºäºçŸ¥è¯†åº“å›ç­”é—®é¢˜
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›ä¼šè‡ªåŠ¨æ£€ç´¢ï¼‰
            
        Returns:
            Dict: å›ç­”ç»“æœ
        """
        try:
            # å¦‚æœæ²¡æœ‰æä¾›ä¸Šä¸‹æ–‡æ–‡æ¡£ï¼Œå…ˆæ£€ç´¢
            if context_docs is None:
                context_docs = self.retrieve_relevant_documents(question)
            
            if not context_docs:
                return {
                    "answer": "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚æ‚¨å¯ä»¥å°è¯•æ·»åŠ æ›´å¤šç›¸å…³æ–‡æ¡£åˆ°çŸ¥è¯†åº“ä¸­ã€‚",
                    "sources": [],
                    "confidence": 0.0,
                    "retrieved_docs": 0
                }
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(context_docs)
            
            # ç”Ÿæˆå›ç­”
            if self.model:
                answer = self._generate_answer_with_model(question, context)
                confidence = self._estimate_confidence(answer, context_docs)
            else:
                answer = self._generate_simple_answer(question, context_docs)
                confidence = 0.5
            
            # æå–æ¥æºä¿¡æ¯
            sources = self._extract_sources(context_docs)
            
            return {
                "answer": answer,
                "sources": sources,
                "confidence": confidence,
                "retrieved_docs": len(context_docs),
                "context_length": len(context)
            }
            
        except Exception as e:
            return {
                "answer": f"å›ç­”é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "retrieved_docs": 0
            }
    
    def _build_context(self, documents: List[Document]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        context_parts = []
        current_length = 0
        max_length = self.config["max_context_length"]
        
        for i, doc in enumerate(documents):
            source_info = f"[æ¥æº {i+1}: {os.path.basename(doc.metadata.get('source', 'unknown'))}]"
            content = f"{source_info}\n{doc.page_content}\n"
            
            if current_length + len(content) > max_length:
                break
            
            context_parts.append(content)
            current_length += len(content)
        
        return "\n---\n".join(context_parts)
    
    def _generate_answer_with_model(self, question: str, context: str) -> str:
        """ä½¿ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”"""
        try:
            prompt = f"""æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ï¼Œå¹¶åœ¨é€‚å½“ä½ç½®å¼•ç”¨æ¥æºã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜éœ€è¦è¡¥å……å“ªäº›ä¿¡æ¯ã€‚"""

            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼Œä¸“é—¨åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"),
                HumanMessage(content=prompt)
            ]
            
            response = self.model.invoke(messages)
            return response.content
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return f"ç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def _generate_simple_answer(self, question: str, documents: List[Document]) -> str:
        """ç”Ÿæˆç®€å•å›ç­”ï¼ˆå½“æ²¡æœ‰æ¨¡å‹æ—¶ï¼‰"""
        # ç®€å•çš„å…³é”®è¯åŒ¹é…å›ç­”
        question_words = set(question.lower().split())
        relevant_snippets = []
        
        for doc in documents:
            content_words = set(doc.page_content.lower().split())
            if question_words.intersection(content_words):
                snippet = doc.page_content[:200] + "..."
                source = os.path.basename(doc.metadata.get('source', 'unknown'))
                relevant_snippets.append(f"æ¥è‡ª {source}: {snippet}")
        
        if relevant_snippets:
            return "åŸºäºçŸ¥è¯†åº“ä¸­çš„ç›¸å…³ä¿¡æ¯ï¼š\n\n" + "\n\n".join(relevant_snippets[:3])
        else:
            return "æœªæ‰¾åˆ°ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚"
    
    def _estimate_confidence(self, answer: str, documents: List[Document]) -> float:
        """ä¼°ç®—å›ç­”ç½®ä¿¡åº¦"""
        try:
            # ç®€å•çš„ç½®ä¿¡åº¦ä¼°ç®—é€»è¾‘
            if "ä¸çŸ¥é“" in answer or "æ²¡æœ‰ä¿¡æ¯" in answer or "æ— æ³•å›ç­”" in answer:
                return 0.2
            
            avg_score = sum(doc.metadata.get("relevance_score", 0.5) for doc in documents) / len(documents)
            
            # åŸºäºæ£€ç´¢æ–‡æ¡£æ•°é‡å’Œå¹³å‡ç›¸ä¼¼åº¦è°ƒæ•´ç½®ä¿¡åº¦
            doc_count_factor = min(1.0, len(documents) / 3)
            confidence = avg_score * doc_count_factor
            
            return min(0.95, max(0.1, confidence))
            
        except Exception:
            return 0.5
    
    def _extract_sources(self, documents: List[Document]) -> List[Dict[str, Any]]:
        """æå–æ¥æºä¿¡æ¯"""
        sources = []
        seen_sources = set()
        
        for doc in documents:
            source_path = doc.metadata.get("source", "unknown")
            if source_path not in seen_sources:
                sources.append({
                    "file_name": os.path.basename(source_path),
                    "file_path": source_path,
                    "chunk_index": doc.metadata.get("chunk_index", 0),
                    "relevance_score": doc.metadata.get("relevance_score", 0.0)
                })
                seen_sources.add(source_path)
        
        return sources
    
    def get_knowledge_base_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = self.vector_db.get_stats()
            documents = self.vector_db.list_documents()
            
            return {
                "total_documents": stats.get("total_documents", 0),
                "total_chunks": stats.get("total_chunks", 0),
                "embedding_model": stats.get("embedding_model", "unknown"),
                "documents": documents,
                "supported_formats": self.doc_processor.get_supported_formats()
            }
        except Exception as e:
            return {"error": f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"}
    
    def delete_document_from_knowledge_base(self, file_path: str) -> Dict[str, Any]:
        """ä»çŸ¥è¯†åº“åˆ é™¤æ–‡æ¡£"""
        try:
            success = self.vector_db.delete_document(file_path)
            if success:
                return {
                    "success": True,
                    "message": f"æˆåŠŸåˆ é™¤æ–‡æ¡£: {os.path.basename(file_path)}"
                }
            else:
                return {
                    "success": False,
                    "error": "åˆ é™¤æ–‡æ¡£å¤±è´¥"
                }
        except Exception as e:
            return {
                "success": False,
                "error": f"åˆ é™¤æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}"
            }
    
    def clear_knowledge_base(self) -> Dict[str, Any]:
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        try:
            success = self.vector_db.clear_all()
            if success:
                return {
                    "success": True,
                    "message": "çŸ¥è¯†åº“å·²æ¸…ç©º"
                }
            else:
                return {
                    "success": False,
                    "error": "æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥"
                }
        except Exception as e:
            return {
                "success": False,
                "error": f"æ¸…ç©ºçŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}"
            }


# å®šä¹‰è¾“å…¥æ¨¡å‹
class EmptyInput(BaseModel):
    """ç©ºè¾“å…¥æ¨¡å‹ï¼Œç”¨äºä¸éœ€è¦å‚æ•°çš„å·¥å…·"""
    pass

class FilePathInput(BaseModel):
    """æ–‡ä»¶è·¯å¾„è¾“å…¥æ¨¡å‹"""
    file_path: str = Field(description="æ–‡ä»¶è·¯å¾„")

class DirectoryInput(BaseModel):
    """ç›®å½•è¾“å…¥æ¨¡å‹"""
    directory_path: str = Field(description="ç›®å½•è·¯å¾„")
    recursive: bool = Field(default=True, description="æ˜¯å¦é€’å½’å¤„ç†å­ç›®å½•")

class QuestionInput(BaseModel):
    """é—®é¢˜è¾“å…¥æ¨¡å‹"""
    question: str = Field(description="è¦è¯¢é—®çš„é—®é¢˜")


def create_rag_tools(model=None) -> List[Tool]:
    """
    åˆ›å»ºRAGç›¸å…³çš„å·¥å…·é›†åˆ
    
    Args:
        model: è¯­è¨€æ¨¡å‹å®ä¾‹
        
    Returns:
        List[Tool]: RAGå·¥å…·åˆ—è¡¨
    """
    # åˆ›å»ºåŸºäº API åˆ†å‰²å†³ç­–çš„ DocumentProcessorï¼ˆä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®ï¼‰
    api_client, api_model = create_api_client(
        api_key="ollama", 
        base_url="http://localhost:11434/v1", 
        model_name="qwen3:1.7b"
        )

    doc_processor = create_document_processor(
        meta_chunking_strategy="prob_subtract",  # ä½¿ç”¨ API å†³ç­–ï¼ˆå¤±è´¥è‡ªåŠ¨å›é€€é»˜è®¤é€»è¾‘ï¼‰
        api_client=api_client,
        api_model=api_model
    )

    # åˆ›å»ºRAGæ£€ç´¢å™¨å®ä¾‹ï¼ˆæ³¨å…¥å®šåˆ¶çš„æ–‡æ¡£å¤„ç†å™¨ï¼‰
    rag_retriever = RAGRetriever(model=model, doc_processor=doc_processor)
    
    def add_document_to_rag(file_path: str) -> str:
        """æ·»åŠ æ–‡æ¡£åˆ°RAGçŸ¥è¯†åº“"""
        result = rag_retriever.add_document_to_knowledge_base(file_path)
        
        if result["success"]:
            return f"âœ… {result['message']}\nğŸ“„ æ–‡ä»¶: {os.path.basename(result['file_path'])}\nğŸ“Š æ·»åŠ äº† {result['chunks_added']} ä¸ªæ–‡æ¡£å—\nğŸ—‚ï¸ çŸ¥è¯†åº“ç°æœ‰ {result['total_documents']} ä¸ªæ–‡æ¡£ï¼Œå…± {result['total_chunks']} ä¸ªå—"
        else:
            return f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {result['error']}"
    
    def add_directory_to_rag(directory_path: str, recursive: bool = True) -> str:
        """æ‰¹é‡æ·»åŠ ç›®å½•ä¸‹çš„æ–‡æ¡£åˆ°RAGçŸ¥è¯†åº“"""
        result = rag_retriever.add_directory_to_knowledge_base(directory_path, recursive)
        
        if result["success"]:
            message = f"âœ… {result['message']}\nğŸ“ ç›®å½•: {result['directory_path']}\nğŸ“„ å¤„ç†äº† {result['processed_files']} ä¸ªæ–‡ä»¶\nğŸ“Š æ·»åŠ äº† {result['total_chunks_added']} ä¸ªæ–‡æ¡£å—\nğŸ—‚ï¸ çŸ¥è¯†åº“ç°æœ‰ {result['total_documents']} ä¸ªæ–‡æ¡£ï¼Œå…± {result['total_chunks']} ä¸ªå—"
            
            if result.get("errors"):
                message += f"\nâš ï¸ é”™è¯¯: {', '.join(result['errors'])}"
            
            return message
        else:
            return f"âŒ å¤„ç†ç›®å½•å¤±è´¥: {result['error']}"
    
    def rag_question_answer(question: str) -> str:
        """åŸºäºRAGçŸ¥è¯†åº“å›ç­”é—®é¢˜"""
        result = rag_retriever.answer_question(question)
        
        response = f"ğŸ¤– RAGå›ç­”:\n{result['answer']}\n"
        
        if result["sources"]:
            response += f"\nğŸ“š å‚è€ƒæ¥æº:\n"
            for i, source in enumerate(result["sources"][:3], 1):
                response += f"{i}. {source['file_name']} (ç›¸ä¼¼åº¦: {source['relevance_score']:.2f})\n"
        
        response += f"\nğŸ“Š æ£€ç´¢äº† {result['retrieved_docs']} ä¸ªç›¸å…³æ–‡æ¡£å—"
        response += f"\nğŸ¯ ç½®ä¿¡åº¦: {result['confidence']:.2f}"
        
        return response
    
    def get_rag_stats(input_data="") -> str:
        """è·å–RAGçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        stats = rag_retriever.get_knowledge_base_stats()
        
        if "error" in stats:
            return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {stats['error']}"
        
        response = f"ğŸ“Š RAGçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯:\n"
        response += f"ğŸ“„ æ–‡æ¡£æ€»æ•°: {stats['total_documents']}\n"
        response += f"ğŸ“‹ æ–‡æ¡£å—æ€»æ•°: {stats['total_chunks']}\n"
        response += f"ğŸ§  åµŒå…¥æ¨¡å‹: {stats['embedding_model']}\n"
        response += f"ğŸ“ æ”¯æŒæ ¼å¼: {', '.join(stats['supported_formats'])}\n"
        
        if stats['documents']:
            response += f"\nğŸ“‚ å·²ç´¢å¼•çš„æ–‡æ¡£:\n"
            for doc in stats['documents'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                response += f"â€¢ {doc['file_name']} ({doc['chunks']} å—)\n"
            
            if len(stats['documents']) > 5:
                response += f"... è¿˜æœ‰ {len(stats['documents']) - 5} ä¸ªæ–‡æ¡£\n"
        
        return response
    
    def delete_rag_document(file_path: str) -> str:
        """ä»RAGçŸ¥è¯†åº“åˆ é™¤æ–‡æ¡£"""
        result = rag_retriever.delete_document_from_knowledge_base(file_path)
        
        if result["success"]:
            return f"âœ… {result['message']}"
        else:
            return f"âŒ åˆ é™¤å¤±è´¥: {result['error']}"
    
    def clear_rag_knowledge_base(input_data="") -> str:
        """æ¸…ç©ºRAGçŸ¥è¯†åº“"""
        result = rag_retriever.clear_knowledge_base()
        
        if result["success"]:
            return f"âœ… {result['message']}"
        else:
            return f"âŒ æ¸…ç©ºå¤±è´¥: {result['error']}"
    
    # åˆ›å»ºå·¥å…·åˆ—è¡¨
    tools = [
        Tool(
            name="add_document_to_rag",
            description="""å°†æ–‡æ¡£æ·»åŠ åˆ°RAGçŸ¥è¯†åº“ä¸­ï¼Œç”¨äºåç»­çš„æ–‡æ¡£é—®ç­”ã€‚
            
æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .txt, .md, .py, .js, .java, .cpp, .html, .json, .csv, .pdf, .docx ç­‰
            
ä½¿ç”¨åœºæ™¯:
- æ·»åŠ æŠ€æœ¯æ–‡æ¡£ã€è¯´æ˜ä¹¦åˆ°çŸ¥è¯†åº“
- å»ºç«‹é¡¹ç›®ç›¸å…³çš„é—®ç­”ç³»ç»Ÿ  
- åˆ›å»ºä¸ªäººçŸ¥è¯†ç®¡ç†ç³»ç»Ÿ

å‚æ•°:
- file_path: è¦æ·»åŠ çš„æ–‡ä»¶è·¯å¾„ï¼ˆå¿…éœ€ï¼‰

ç¤ºä¾‹: add_document_to_rag("./README.md")""",
            func=add_document_to_rag
        ),
        
        Tool(
            name="add_directory_to_rag",
            description="""æ‰¹é‡å°†ç›®å½•ä¸‹çš„æ‰€æœ‰æ”¯æŒæ–‡æ¡£æ·»åŠ åˆ°RAGçŸ¥è¯†åº“ã€‚
            
ä½¿ç”¨åœºæ™¯:
- æ‰¹é‡å¯¼å…¥é¡¹ç›®æ–‡æ¡£
- å»ºç«‹å®Œæ•´çš„æ–‡æ¡£åº“
- å¤„ç†å¤§é‡å†å²æ–‡æ¡£

å‚æ•°:
- directory_path: ç›®å½•è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- recursive: æ˜¯å¦é€’å½’å¤„ç†å­ç›®å½•ï¼Œé»˜è®¤Trueï¼ˆå¯é€‰ï¼‰

ç¤ºä¾‹: add_directory_to_rag("./docs/")""",
            func=add_directory_to_rag
        ),
        
        Tool(
            name="rag_question_answer",
            description="""åŸºäºRAGçŸ¥è¯†åº“å›ç­”é—®é¢˜ã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ç”Ÿæˆå›ç­”ã€‚
            
ä½¿ç”¨åœºæ™¯:
- æ ¹æ®æ–‡æ¡£å†…å®¹å›ç­”æŠ€æœ¯é—®é¢˜
- å¿«é€ŸæŸ¥æ‰¾é¡¹ç›®ç›¸å…³ä¿¡æ¯
- åŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”

å‚æ•°:
- question: è¦è¯¢é—®çš„é—®é¢˜ï¼ˆå¿…éœ€ï¼‰

ç¤ºä¾‹: rag_question_answer("å¦‚ä½•å®‰è£…è¿™ä¸ªé¡¹ç›®ï¼Ÿ")""",
            func=rag_question_answer
        ),
        
        Tool(
            name="get_rag_stats",
            description="""è·å–RAGçŸ¥è¯†åº“çš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡æ¡£æ•°é‡ã€å—æ•°é‡ã€å·²ç´¢å¼•æ–‡æ¡£åˆ—è¡¨ç­‰ã€‚
            
ä½¿ç”¨åœºæ™¯:
- æŸ¥çœ‹çŸ¥è¯†åº“çŠ¶æ€
- äº†è§£å·²æ·»åŠ çš„æ–‡æ¡£
- ç›‘æ§ç³»ç»Ÿè¿è¡ŒçŠ¶æ€

æ— éœ€å‚æ•°

ç¤ºä¾‹: get_rag_stats()""",
            func=get_rag_stats
        ),
        
        Tool(
            name="delete_rag_document", 
            description="""ä»RAGçŸ¥è¯†åº“ä¸­åˆ é™¤æŒ‡å®šæ–‡æ¡£ã€‚
            
ä½¿ç”¨åœºæ™¯:
- ç§»é™¤è¿‡æ—¶çš„æ–‡æ¡£
- æ¸…ç†é”™è¯¯æ·»åŠ çš„æ–‡ä»¶
- ç®¡ç†çŸ¥è¯†åº“å†…å®¹

å‚æ•°:
- file_path: è¦åˆ é™¤çš„æ–‡ä»¶è·¯å¾„ï¼ˆå¿…éœ€ï¼‰

ç¤ºä¾‹: delete_rag_document("./old_doc.md")""",
            func=delete_rag_document
        ),
        
        Tool(
            name="clear_rag_knowledge_base",
            description="""æ¸…ç©ºæ•´ä¸ªRAGçŸ¥è¯†åº“ï¼Œåˆ é™¤æ‰€æœ‰å·²æ·»åŠ çš„æ–‡æ¡£ã€‚
            
âš ï¸ è­¦å‘Š: æ­¤æ“ä½œä¸å¯é€†ï¼Œä¼šåˆ é™¤æ‰€æœ‰æ•°æ®ï¼

ä½¿ç”¨åœºæ™¯:
- é‡æ–°å»ºç«‹çŸ¥è¯†åº“
- æ¸…ç†æµ‹è¯•æ•°æ®
- è§£å†³æ•°æ®é—®é¢˜

æ— éœ€å‚æ•°

ç¤ºä¾‹: clear_rag_knowledge_base()""",
            func=clear_rag_knowledge_base
        )
    ]
    
    return tools

def main():
    # åˆå§‹åŒ–å·¥å…·é›†åˆ
    tools = create_rag_tools(model=None)  # å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œå¯ä»¥ä¼  None

    # è·å–å…·ä½“å·¥å…·å‡½æ•°
    add_document_to_rag = next(t.func for t in tools if t.name == "add_document_to_rag")
    add_directory_to_rag = next(t.func for t in tools if t.name == "add_directory_to_rag")
    rag_question_answer = next(t.func for t in tools if t.name == "rag_question_answer")
    get_rag_stats = next(t.func for t in tools if t.name == "get_rag_stats")
    delete_rag_document = next(t.func for t in tools if t.name == "delete_rag_document")
    clear_rag_knowledge_base = next(t.func for t in tools if t.name == "clear_rag_knowledge_base")

    # æµ‹è¯•å•ä¸ªæ–‡æ¡£æ·»åŠ 
    file_path = "./doc/rag_usage_guide.md"
    print(add_document_to_rag(file_path))

    # æµ‹è¯•ç›®å½•æ·»åŠ 
    # dir_path = "./docs/"
    # print(add_directory_to_rag(dir_path))

    # æµ‹è¯•é—®é¢˜å›ç­”
    question = "è¯åˆ¸æ³•è§„å®šäº†å“ªäº›ä¿¡æ¯æŠ«éœ²ä¹‰åŠ¡ï¼Ÿ"
    print(rag_question_answer(question))

    # æŸ¥çœ‹çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
    print(get_rag_stats())

    # æµ‹è¯•åˆ é™¤æ–‡æ¡£
    # print(delete_rag_document(file_path))

    # æµ‹è¯•æ¸…ç©ºçŸ¥è¯†åº“
    # print(clear_rag_knowledge_base())

if __name__ == "__main__":
    main()